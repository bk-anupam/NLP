{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict tags on StackOverflow with linear models\r\n",
    "\r\n",
    "In this assignment you will learn how to predict tags for posts from [StackOverflow](https://stackoverflow.com). To solve this task you will use multilabel classification approach.\r\n",
    "\r\n",
    "### Libraries\r\n",
    "\r\n",
    "In this task you will need the following libraries:\r\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\r\n",
    "- [Pandas](https://pandas.pydata.org) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\r\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\r\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training, validation and test data (do this if the week1/data folder is empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data\\train.tsv is already downloaded.\n",
      "File data\\validation.tsv is already downloaded.\n",
      "File data\\test.tsv is already downloaded.\n",
      "File data\\text_prepare_tests.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\r\n",
    "sys.path.append(\"..\")\r\n",
    "from common.download_utils import download_week1_resources\r\n",
    "\r\n",
    "download_week1_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\r\n",
    "For this and most of the following assignments you will need to use a list of stop words. It can be downloaded from nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anupam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set is provided for Coursera's grading and doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  tags\n",
       "0                How to draw a stacked dotplot in R?                   [r]\n",
       "1  mysql select all records where a datetime fiel...          [php, mysql]\n",
       "2             How to terminate windows phone 8.1 app                  [c#]\n",
       "3  get current time in a specific country via jquery  [javascript, jquery]\n",
       "4                      Configuring Tomcat to Use SSL                [java]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from ast import literal_eval\r\n",
    "\r\n",
    "def read_data(filename):\r\n",
    "    data = pd.read_csv(filename, sep='\\t')\r\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\r\n",
    "    return data\r\n",
    "\r\n",
    "train_data = read_data('data/train.tsv') \r\n",
    "val_data = read_data('data/validation.tsv')   \r\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t')\r\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *title* column contains titles of the posts and *tags* column contains the tags. It could be noticed that a number of tags for a post is not fixed and could be as many as necessary.\r\n",
    "\r\n",
    "For a more comfortable usage, initialize *X_train*, *X_val*, *X_test*, *y_train*, *y_val*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) (100000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data['title'].values\r\n",
    "y_train = train_data['tags'].values\r\n",
    "X_val = val_data['title'].values\r\n",
    "y_val = val_data['tags'].values\r\n",
    "X_test = test_data['title'].values\r\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5?*, *\"Flip*, etc. To prevent the problems, it's usually useful to prepare the data somehow. In this task you'll write a function, which will be also used in the other assignments. \r\n",
    "\r\n",
    "**Task 1 (TextPrepare).** Implement the function *text_prepare* following the instructions. After that, run the function *test_text_prepare* to test it on tiny cases and submit it to Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\r\n",
    "\r\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\r\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\r\n",
    "STOPWORDS = set(stopwords.words('english'))\r\n",
    "\r\n",
    "def text_prepare(text):\r\n",
    "    \"\"\"\r\n",
    "        text: a string        \r\n",
    "        return: modified initial string\r\n",
    "    \"\"\"\r\n",
    "    text = text.lower() # lowercase text\r\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\r\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\r\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\r\n",
    "    return text.strip()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "def test_text_prepare():\r\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\r\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\r\n",
    "    answers = [\"sql server equivalent excels choose function\", \r\n",
    "               \"free c++ memory vectorint arr\"]\r\n",
    "    for ex, ans in zip(examples, answers):\r\n",
    "        if text_prepare(ex) != ans:\r\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\r\n",
    "    return 'Basic tests are passed.'    \r\n",
    "    \r\n",
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your implementation for questions from file *text_prepare_tests.tsv* to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_text</th>\n",
       "      <th>test_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQLite/PHP read-only?</td>\n",
       "      <td>sqlite php readonly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creating Multiple textboxes dynamically</td>\n",
       "      <td>creating multiple textboxes dynamically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that, self or me — which one to prefer in JavaScript?</td>\n",
       "      <td>self one prefer javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Save PHP date string into MySQL database as timestamp</td>\n",
       "      <td>save php date string mysql database timestamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I can fill my DropDownList with Data from a XML File in my ASP.NET Application</td>\n",
       "      <td>fill dropdownlist data xml file aspnet application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Programmatically trigger a jQuery-UI draggable's \"drag\" event</td>\n",
       "      <td>programmatically trigger jqueryui draggables drag event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How to get the value of a method argument via reflection in Java?</td>\n",
       "      <td>get value method argument via reflection java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Knockout maping.fromJS for observableArray from json object. Data gets lost</td>\n",
       "      <td>knockout mapingfromjs observablearray json object data gets lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Facebook Connect from Localhost, doing some weird stuff</td>\n",
       "      <td>facebook connect localhost weird stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fullcalendar prev / next click</td>\n",
       "      <td>fullcalendar prev next click</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            test_text  \\\n",
       "0                                                               SQLite/PHP read-only?   \n",
       "1                                             Creating Multiple textboxes dynamically   \n",
       "2                               that, self or me — which one to prefer in JavaScript?   \n",
       "3                               Save PHP date string into MySQL database as timestamp   \n",
       "4  How I can fill my DropDownList with Data from a XML File in my ASP.NET Application   \n",
       "5                       Programmatically trigger a jQuery-UI draggable's \"drag\" event   \n",
       "6                   How to get the value of a method argument via reflection in Java?   \n",
       "7         Knockout maping.fromJS for observableArray from json object. Data gets lost   \n",
       "8                             Facebook Connect from Localhost, doing some weird stuff   \n",
       "9                                                      fullcalendar prev / next click   \n",
       "\n",
       "                                                test_text_processed  \n",
       "0                                               sqlite php readonly  \n",
       "1                           creating multiple textboxes dynamically  \n",
       "2                                        self one prefer javascript  \n",
       "3                     save php date string mysql database timestamp  \n",
       "4                fill dropdownlist data xml file aspnet application  \n",
       "5           programmatically trigger jqueryui draggables drag event  \n",
       "6                     get value method argument via reflection java  \n",
       "7  knockout mapingfromjs observablearray json object data gets lost  \n",
       "8                            facebook connect localhost weird stuff  \n",
       "9                                      fullcalendar prev next click  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display full text in column\r\n",
    "pd.set_option('display.max_colwidth', None)\r\n",
    "df_test_text = pd.read_csv('data/text_prepare_tests.tsv', '\\t', names=['test_text'])\r\n",
    "df_test_text['test_text_processed'] = df_test_text['test_text'].apply(text_prepare)\r\n",
    "df_test_text.head(10)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_prepare(text) for text in X_train])\r\n",
    "X_val = np.array([text_prepare(text) for text in X_val])\r\n",
    "X_test = np.array([text_prepare(text) for text in X_test])\r\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tag and for each word calculate how many times they occur in the train corpus. \r\n",
    "\r\n",
    "**Task 2 (WordsTagsCount).** Find 3 most popular tags and 3 most popular words in the train data and submit the results to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 most popular words in train data are:\n",
      "'using' with a count of 8278\n",
      "'php' with a count of 5614\n",
      "'java' with a count of 5501\n"
     ]
    }
   ],
   "source": [
    "def get_word_count(data):\r\n",
    "    word_count = {}\r\n",
    "    for text in data:    \r\n",
    "        for word in text.split():\r\n",
    "            if word in word_count.keys():\r\n",
    "                word_count[word] += 1\r\n",
    "            else:\r\n",
    "                word_count[word] = 1\r\n",
    "    return word_count\r\n",
    "\r\n",
    "train_word_freq = get_word_count(X_train)\r\n",
    "\r\n",
    "def get_topn_dictitems_byvalue(dict_data, n):\r\n",
    "    freq_word = [(value, key) for key, value in dict_data.items()]\r\n",
    "    freq_word.sort(reverse=True, key=lambda k: k[0])\r\n",
    "    return freq_word[:n]\r\n",
    "\r\n",
    "print('3 most popular words in train data are:')\r\n",
    "for item in get_topn_dictitems_byvalue(train_word_freq, 3):\r\n",
    "    print(f\"'{item[1]}' with a count of {item[0]}\")\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31497\n"
     ]
    }
   ],
   "source": [
    "print(len(train_word_freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 most popular tags in train data are:\n",
      "'javascript' with a count of 19078\n",
      "'c#' with a count of 19077\n",
      "'java' with a count of 18661\n"
     ]
    }
   ],
   "source": [
    "def get_tag_count(tag_data):\r\n",
    "    tag_count = {}\r\n",
    "    for tags in tag_data:\r\n",
    "        for tag in tags:\r\n",
    "            if tag in tag_count.keys():\r\n",
    "                tag_count[tag] += 1\r\n",
    "            else:\r\n",
    "                tag_count[tag] = 1\r\n",
    "    return tag_count\r\n",
    "    \r\n",
    "tag_freq = get_tag_count(y_train)    \r\n",
    "print('3 most popular tags in train data are:')\r\n",
    "for item in get_topn_dictitems_byvalue(tag_freq, 3):\r\n",
    "    print(f\"'{item[1]}' with a count of {item[0]}\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\r\n",
    "\r\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\r\n",
    "\r\n",
    "#### Bag of words\r\n",
    "\r\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\r\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\r\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\r\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\r\n",
    "\r\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \r\n",
    "\r\n",
    "    ['hi', 'you', 'me', 'are']\r\n",
    "\r\n",
    "Then we need to numerate them, for example, like this: \r\n",
    "\r\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\r\n",
    "\r\n",
    "And we have the text, which we want to transform to the vector:\r\n",
    "\r\n",
    "    'hi how are you'\r\n",
    "\r\n",
    "For this text we create a corresponding zero vector \r\n",
    "\r\n",
    "    [0, 0, 0, 0]\r\n",
    "    \r\n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\r\n",
    "\r\n",
    "    'hi':  [1, 0, 0, 0]\r\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\r\n",
    "    'are': [1, 0, 0, 1]\r\n",
    "    'you': [1, 1, 0, 1]\r\n",
    "\r\n",
    "The resulting vector will be \r\n",
    "\r\n",
    "    [1, 1, 0, 1]\r\n",
    "   \r\n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\r\n",
    "# we use the most frequent 5000 words as our vocabulary\r\n",
    "VOCAB = [item[1] for item in get_topn_dictitems_byvalue(train_word_freq, DICT_SIZE)]\r\n",
    "WORDS_TO_INDEX = {word: i for i, word in enumerate(VOCAB)}\r\n",
    "\r\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\r\n",
    "    \"\"\"\r\n",
    "        text: a string\r\n",
    "        dict_size: size of the dictionary\r\n",
    "        \r\n",
    "        return a vector which is a bag-of-words representation of 'text'\r\n",
    "    \"\"\"\r\n",
    "    result_vector = np.zeros(dict_size)\r\n",
    "    for word in text.split():\r\n",
    "        if word in words_to_index.keys():\r\n",
    "            result_vector[words_to_index[word]] += 1\r\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "def test_my_bag_of_words():\r\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\r\n",
    "    examples = ['hi how are you']\r\n",
    "    answers = [[1, 1, 0, 1]]\r\n",
    "    for ex, ans in zip(examples, answers):\r\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\r\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\r\n",
    "    return 'Basic tests are passed.'\r\n",
    "\r\n",
    "print(test_my_bag_of_words())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the implemented function to all samples (this might take up to a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\r\n",
    "\r\n",
    "X_train_compressed = csr_matrix([my_bag_of_words(text, WORDS_TO_INDEX, 5000) for text in X_train])\r\n",
    "X_val_compressed = csr_matrix([my_bag_of_words(text, WORDS_TO_INDEX, 5000) for text in X_val])\r\n",
    "X_test_compressed = csr_matrix([my_bag_of_words(text, WORDS_TO_INDEX, 5000) for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_row_nonzero_count(csr_matrix, row_index):\r\n",
    "    row = csr_matrix[row_index].toarray()[0]    \r\n",
    "    non_zero_items = [(i, item) for i, item in enumerate(row) if item != 0]\r\n",
    "    return len(non_zero_items)\r\n",
    "\r\n",
    "get_row_nonzero_count(X_train_compressed, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of custom code one case use sklearn's implementation to convert input text data to bag of words sparse matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5000)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "vectorizer = CountVectorizer(vocabulary=VOCAB)\r\n",
    "X_train_csr = vectorizer.transform(X_train)\r\n",
    "print(X_train_csr.shape)\r\n",
    "nonzero_terms = vectorizer.inverse_transform(X_train_csr[10])\r\n",
    "print(len(nonzero_terms[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\r\n",
    "\r\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \r\n",
    "\r\n",
    "Implement function *tfidf_features* using class [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 17778)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))\r\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\r\n",
    "X_train_tfidf.shape\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done text preprocessing, always have a look at the results. Be very careful at this step, because the performance of future models will drastically depend on it. \r\n",
    "\r\n",
    "In this case, check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no c++ in vocabulary\n",
      "no c# in vocabulary\n"
     ]
    }
   ],
   "source": [
    "if 'c++' not in tfidf_vectorizer.vocabulary_.keys():\r\n",
    "    print('no c++ in vocabulary')\r\n",
    "if 'c#' not in tfidf_vectorizer.vocabulary_.keys():\r\n",
    "    print('no c# in vocabulary')    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ff06204c0662b9359ef4233b0e8cfcc016e07736dbe455d1edaa8487878aae2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}