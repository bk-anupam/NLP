{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's craft a Retrieval-Augmented Generation (RAG) application in Python using Langchain to query PDF documents. This will involve several steps:\n",
    "\n",
    "1.  **Loading PDF Documents:** We'll use `PyPDFLoader` from Langchain to load the PDF files.\n",
    "2.  **Text Splitting:** We'll split the loaded text into smaller chunks using `RecursiveCharacterTextSplitter` to manage context size for the language model.\n",
    "3.  **Embedding Generation:** We'll generate embeddings for these chunks using a model like OpenAI's `OpenAIEmbeddings`.\n",
    "4.  **Vector Store Creation:** We'll store the embeddings in a vector database (e.g., Chroma).\n",
    "5.  **Retrieval:**  We'll retrieve relevant chunks from the vector store based on a user's query.\n",
    "6.  **Language Model Integration:** We'll use a language model (e.g., OpenAI's GPT) to generate an answer based on the retrieved context and the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# INDEXING\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Loads a PDF document from the given path.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_text(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Splits the documents into chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    return texts\n",
    "\n",
    "def create_embeddings():\n",
    "    \"\"\"Creates embeddings using HuggingFaceEmbeddings.  Good for Gemini\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # Or another suitable model\n",
    "    return embeddings\n",
    "\n",
    "def create_vectorstore(texts, embeddings, persist_directory=\"chroma_db\"):\n",
    "    \"\"\"Creates a Chroma vectorstore from the texts and embeddings.\"\"\"\n",
    "    vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\n",
    "    vectordb.persist()  # Persist the vectorstore to disk\n",
    "    return vectordb\n",
    "\n",
    "def build_index(pdf_path, chunk_size=1000, chunk_overlap=200, persist_directory=\"chroma_db\"):\n",
    "    \"\"\"Builds the index from the PDF document.\"\"\"\n",
    "    documents = load_pdf(pdf_path)\n",
    "    texts = split_text(documents, chunk_size, chunk_overlap)\n",
    "    embeddings = create_embeddings()\n",
    "    vectordb = create_vectorstore(texts, embeddings, persist_directory)\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# QUERYING\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_existing_index(persist_directory=\"chroma_db\"):\n",
    "    \"\"\"Loads an existing Chroma vectorstore from disk.\"\"\"\n",
    "    embeddings = create_embeddings() # Make sure you use the same embedding model used during indexing\n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vectordb\n",
    "\n",
    "def query_index(vectordb, query, chain_type=\"stuff\", k=4, model_name=\"gemini-2.0-flash\"):  # Added model_name\n",
    "    \"\"\"Queries the vectorstore and returns the answer.\"\"\"\n",
    "    #llm = GoogleGenerativeAI(model=model_name)  #Using generative model\n",
    "    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.3) #Using chat generative model\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=chain_type,\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": k}),\n",
    "        return_source_documents=True  # Optional:  Return the source documents used for the answer\n",
    "    )\n",
    "    result = qa({\"query\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29554/2287342569.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # Or another suitable model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811a5f1fa860472e8423958636277d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c646101a2834ba7aed268e48e815d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72843697476f489683388c25e31d8ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1226b6985005487f88b454474e02c5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae4f28ead86446a8d841e30f4a1174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85a5a8ad8d64524b23aa60dcabc425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e4f441d87641a5a5f399b0e72acae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5030cf54911545f7877fa0e01ba69935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c334b18ecd416a8fafd067cacea1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc89c381f3314be69240974ef71f63cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd69bf277e544c2b9a058b0959d8eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index creation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29554/2287342569.py:21: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()  # Persist the vectorstore to disk\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"./data/murli-2025-02-21.pdf\"  # Replace with your PDF file path.  Example:  \"my_document.pdf\"\n",
    "    persist_directory = \"chroma_db\"  # Directory to store the Chroma database\n",
    "\n",
    "    # 1. Indexing (Create the vectorstore)\n",
    "    #    - Only needs to be done once (or when the PDF content changes)\n",
    "    if not os.path.exists(persist_directory):  # Only build if the database doesn't exist\n",
    "        print(\"Creating new index...\")\n",
    "        vectordb = build_index(pdf_file_path, persist_directory=persist_directory)\n",
    "        print(\"Index creation complete.\")\n",
    "    else:\n",
    "        print(\"Loading existing index...\")\n",
    "        vectordb = load_existing_index(persist_directory)\n",
    "        print(\"Index loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the main concepts discussed in the document?\n",
      "Answer: The document discusses the following main concepts:\n",
      "\n",
      "*   **The Father as the giver of inheritance and the one who takes everyone back home:** The Father is the one who provides the inheritance and guides souls back to the \"home of the Father\" and the \"home of souls,\" which is the abode of silence.\n",
      "*   **Earning an imperishable income:** The Father enables individuals to earn an imperishable income for the future, contrasting with the perishable income provided by gurus and holy men.\n",
      "*   **Understanding and imbibing knowledge:** Sensible children are able to imbibe the knowledge given by the Father.\n",
      "*   **Remembering the Father and the inheritance:** The main thing is to remember the Father and the inheritance. The essence that remains is “Manmanabhav”.\n",
      "*   **The Father as the Ocean of Knowledge and the Creator:** The Father is the Ocean of Knowledge and the Creator, not Shri Krishna.\n",
      "*   **Establishing a kingdom and claiming a high status:** The Father has come to enable individuals to claim a high status, and this kingdom is being established according to the drama plan.\n"
     ]
    }
   ],
   "source": [
    "# 2. Querying (Use the vectorstore to answer questions)\n",
    "query = \"What are the main concepts discussed in the document?\"  # Replace with your query\n",
    "result = query_index(vectordb, query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
