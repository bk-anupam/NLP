{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.819915Z","iopub.status.busy":"2022-01-18T06:07:52.819641Z","iopub.status.idle":"2022-01-18T06:07:52.828139Z","shell.execute_reply":"2022-01-18T06:07:52.827448Z","shell.execute_reply.started":"2022-01-18T06:07:52.819883Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import re\n","import nltk\n","import torch.nn as nn\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import TweetTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, set_seed\n","from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n","from sklearn.model_selection import StratifiedKFold\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.830264Z","iopub.status.busy":"2022-01-18T06:07:52.829954Z","iopub.status.idle":"2022-01-18T06:07:52.840138Z","shell.execute_reply":"2022-01-18T06:07:52.839328Z","shell.execute_reply.started":"2022-01-18T06:07:52.830226Z"},"trusted":true},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"markdown","metadata":{},"source":["### Configuration for training"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.841990Z","iopub.status.busy":"2022-01-18T06:07:52.841534Z","iopub.status.idle":"2022-01-18T06:07:52.852272Z","shell.execute_reply":"2022-01-18T06:07:52.851307Z","shell.execute_reply.started":"2022-01-18T06:07:52.841955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["class MODEL_EVAL_METRIC:\n","    accuracy = \"accuracy\"\n","    f1_score = \"f1_score\"\n","\n","class Config:\n","    TWEET_COL = \"processed_text\"\n","    RANDOM_STATE = 42\n","    BATCH_SIZE = 16\n","    OUT_SIZE = 2\n","    NUM_FOLDS = 5\n","    NUM_EPOCHS = 4\n","    NUM_WORKERS = 8\n","    TRANSFORMER_CHECKPOINT = \"bert-base-uncased\"\n","    # The hidden_size of the output of the last layer of the transformer model used\n","    TRANSFORMER_OUT_SIZE = 768\n","    PAD_TOKEN_ID = 0\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n","    FAST_DEV_RUN = False    \n","    PATIENCE = 5    \n","    IS_BIDIRECTIONAL = True\n","    # model hyperparameters\n","    MODEL_HPARAMS = {\n","        \"lr\": 0.00003,\n","    }\n","\n","DATA_PATH = \"./data/\"\n","\n","# For results reproducibility \n","# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n","set_seed(Config.RANDOM_STATE)\n","print(Config.DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["### Load the data"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.854550Z","iopub.status.busy":"2022-01-18T06:07:52.854200Z","iopub.status.idle":"2022-01-18T06:07:52.948090Z","shell.execute_reply":"2022-01-18T06:07:52.947070Z","shell.execute_reply.started":"2022-01-18T06:07:52.854515Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Rows in train.csv = 7613\n","Rows in test.csv = 3263\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location  \\\n","0   1     NaN      NaN   \n","1   4     NaN      NaN   \n","2   5     NaN      NaN   \n","3   6     NaN      NaN   \n","4   7     NaN      NaN   \n","\n","                                                                                                                                    text  \\\n","0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n","1                                                                                                 Forest fire near La Ronge Sask. Canada   \n","2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n","3                                                                      13,000 people receive #wildfires evacuation orders in California    \n","4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv(DATA_PATH + 'train.csv')\n","df_test = pd.read_csv(DATA_PATH + 'test.csv')\n","print(f\"Rows in train.csv = {len(df_train)}\")\n","print(f\"Rows in test.csv = {len(df_test)}\")\n","pd.set_option('display.max_colwidth', None)\n","df_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["### K Fold CV\n","Split the training dataframe into kfolds for cross validation. We do this before any processing is done\n","on the data. We use stratified kfold if the target distribution is unbalanced"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.949722Z","iopub.status.busy":"2022-01-18T06:07:52.949459Z","iopub.status.idle":"2022-01-18T06:07:52.978836Z","shell.execute_reply":"2022-01-18T06:07:52.977998Z","shell.execute_reply.started":"2022-01-18T06:07:52.949684Z"},"trusted":true},"outputs":[],"source":["def strat_kfold_dataframe(df, num_folds=5):\n","    # we create a new column called kfold and fill it with -1\n","    df[\"kfold\"] = -1\n","    # randomize of shuffle the rows of dataframe before splitting is done\n","    df.sample(frac=1, random_state=Config.RANDOM_STATE).reset_index(drop=True)\n","    y = df[\"target\"].values\n","    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE)\n","    # stratification is done on the basis of y labels, a placeholder for X is sufficient\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=y)):\n","        df.loc[val_idx, \"kfold\"] = fold\n","    return df\n","\n","df_train = strat_kfold_dataframe(df_train, num_folds=Config.NUM_FOLDS)            "]},{"cell_type":"markdown","metadata":{},"source":["### Tweet preprocessing"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.980815Z","iopub.status.busy":"2022-01-18T06:07:52.980198Z","iopub.status.idle":"2022-01-18T06:07:52.996249Z","shell.execute_reply":"2022-01-18T06:07:52.995238Z","shell.execute_reply.started":"2022-01-18T06:07:52.980774Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/bk_anupam/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /home/bk_anupam/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:52.998099Z","iopub.status.busy":"2022-01-18T06:07:52.997606Z","iopub.status.idle":"2022-01-18T06:07:53.010758Z","shell.execute_reply":"2022-01-18T06:07:53.009827Z","shell.execute_reply.started":"2022-01-18T06:07:52.998045Z"},"trusted":true},"outputs":[],"source":["punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","def clean_special_chars(text, punct):\n","    for p in punct:\n","        text = text.replace(p, ' ')\n","    return text\n","\n","def process_tweet(df, text, keyword):\n","    lemmatizer = WordNetLemmatizer()    \n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n","    processed_text = []\n","    stop = stopwords.words(\"english\")\n","    for tweet, keyword in zip(df[text], df[keyword]):\n","        tweets_clean = []        \n","        # remove stock market tickers like $GE        \n","        tweet = re.sub(r'\\$\\w*', '', tweet)\n","        # remove old style retweet text \"RT\"\n","        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","        # remove hyperlinks\n","        tweet = re.sub(r'http\\S+', '', tweet)\n","        # remove hashtags\n","        # only removing the hash #, @, ... sign from the word\n","        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n","        tweet = clean_special_chars(tweet, punct)\n","        # remove junk characters which don't have an ascii code\n","        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n","        # tokenize tweets        \n","        tweet_tokens = tokenizer.tokenize(tweet)\n","        for word in tweet_tokens:\n","            # remove stopwords and punctuation\n","            #if (word.isalpha() and len(word) > 2 and word not in stop and word not in string.punctuation):\n","                #stem_word = stemmer.stem(word)  # stemming word            \n","                #lem_word = lemmatizer.lemmatize(word)\n","                #tweets_clean.append(lem_word) \n","                tweets_clean.append(word)\n","        processed_text.append(\" \".join(tweets_clean))        \n","    df['processed_text'] = np.array(processed_text)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:53.022817Z","iopub.status.busy":"2022-01-18T06:07:53.021734Z","iopub.status.idle":"2022-01-18T06:07:54.916387Z","shell.execute_reply":"2022-01-18T06:07:54.915695Z","shell.execute_reply.started":"2022-01-18T06:07:53.022778Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>kfold</th>\n","      <th>processed_text</th>\n","      <th>prcsd_tweet_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>50</th>\n","      <td>73</td>\n","      <td>ablaze</td>\n","      <td>Sheffield Township, Ohio</td>\n","      <td>Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>deputies man shot before brighton home set ablaze</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>74</td>\n","      <td>ablaze</td>\n","      <td>India</td>\n","      <td>Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>man wife get six years jail for setting ablaze niece</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id keyword                  location  \\\n","50  73  ablaze  Sheffield Township, Ohio   \n","51  74  ablaze                     India   \n","\n","                                                                            text  \\\n","50     Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k   \n","51  Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA   \n","\n","    target  kfold                                        processed_text  \\\n","50       1      2     deputies man shot before brighton home set ablaze   \n","51       1      0  man wife get six years jail for setting ablaze niece   \n","\n","    prcsd_tweet_len  \n","50                8  \n","51               10  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Fill in missing values\n","df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n","df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n","process_tweet(df_train, 'text', \"keyword\")\n","process_tweet(df_test, 'text', \"keyword\")\n","# length of the processed tweet\n","df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n","df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\n","df_train.iloc[50:52, :]"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:54.918593Z","iopub.status.busy":"2022-01-18T06:07:54.918209Z","iopub.status.idle":"2022-01-18T06:07:55.676268Z","shell.execute_reply":"2022-01-18T06:07:55.675518Z","shell.execute_reply.started":"2022-01-18T06:07:54.918555Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(Config.TRANSFORMER_CHECKPOINT)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset for transformer model\n","Converts tweets into pytorch dataset compatible with BERT and other transformers"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.677746Z","iopub.status.busy":"2022-01-18T06:07:55.677406Z","iopub.status.idle":"2022-01-18T06:07:55.685637Z","shell.execute_reply":"2022-01-18T06:07:55.684447Z","shell.execute_reply.started":"2022-01-18T06:07:55.677706Z"},"trusted":true},"outputs":[],"source":["# Convert tweets to data that the BERT model understands\n","class TransformerTweetDataset(Dataset):\n","    def __init__(self, tweets, targets, tokenizer, with_labels=True):\n","        self.tweets = tweets\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.with_labels = with_labels\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, item_idx):\n","        inputs = self.tokenizer(self.tweets[item_idx], padding=\"longest\", truncation=True)\n","        item = {\n","            \"input_ids\": torch.LongTensor(inputs[\"input_ids\"]),\n","            \"token_type_ids\": torch.LongTensor(inputs[\"token_type_ids\"]),\n","            \"attention_mask\": torch.LongTensor(inputs[\"attention_mask\"])            \n","        }\n","        if self.with_labels:\n","            targets = self.targets[item_idx]\n","            item[\"labels\"] = torch.LongTensor([targets])\n","        return item"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['id', 'keyword', 'location', 'text', 'target', 'kfold', 'processed_text', 'prcsd_tweet_len']\n","Dataset({\n","    features: ['id', 'keyword', 'location', 'text', 'target', 'kfold', 'processed_text', 'prcsd_tweet_len'],\n","    num_rows: 7613\n","})\n"]}],"source":["from datasets import Dataset\n","\n","# Instead of creating a custom pytorch dataset, we use the HF dataset library\n","ds_train = Dataset.from_pandas(df=df_train)\n","raw_ds_col_names = ds_train.column_names\n","print(raw_ds_col_names)\n","print(ds_train)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Function to tokenize tweets, to be passed to HF datasets.map for tokenization in batches (performed parallely)\n","def tokenize_tweets(tokenizer, with_labels, data_row):\n","    encoding = tokenizer(\n","        text = data_row[\"text\"],\n","        # we don't want to do padding while tokenizing the text (for the entire dataset) but while preparing batches\n","        # using DataCollatorWithPadding. \n","        padding = False,\n","        # If the text length exceeds than what model can handle, truncate the text\n","        truncation = True\n","    )\n","    # For train and validation data the encoding needs to contain the labels as well in addition to input_ids, token_type_ids \n","    # and attention_mask. For test data, labels are not present in encoding.\n","    if with_labels:\n","        encoding[\"labels\"] = data_row[\"target\"]\n","    return encoding"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from functools import partial\n","\n","preprocess_train_data = partial(tokenize_tweets, tokenizer, True)  \n","preprocess_test_data = partial(tokenize_tweets, tokenizer, False)  "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Create fold specific datasets using HF datasets library\n","def get_fold_data_hf(fold, df):\n","    train_df = df[df.kfold != fold].reset_index(drop=True)\n","    valid_df = df[df.kfold == fold].reset_index(drop=True)\n","    ds_train_raw = Dataset.from_pandas(train_df)\n","    ds_valid_raw = Dataset.from_pandas(valid_df)\n","    raw_ds_col_names = ds_train_raw.column_names    \n","    ds_train = ds_train_raw.map(preprocess_train_data, batched=True, batch_size=1000, remove_columns=raw_ds_col_names)\n","    ds_valid = ds_valid_raw.map(preprocess_train_data, batched=True, batch_size=1000, remove_columns=raw_ds_col_names)    \n","    return ds_train, ds_valid"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.687698Z","iopub.status.busy":"2022-01-18T06:07:55.687345Z","iopub.status.idle":"2022-01-18T06:07:55.697251Z","shell.execute_reply":"2022-01-18T06:07:55.696535Z","shell.execute_reply.started":"2022-01-18T06:07:55.687659Z"},"trusted":true},"outputs":[],"source":["def get_fold_data(fold, df, tokenizer, tweet_col=\"text\"):\n","    train_df = df[df.kfold != fold].reset_index(drop=True)\n","    valid_df = df[df.kfold == fold].reset_index(drop=True)\n","    X_train = train_df[tweet_col].values.tolist()\n","    y_train = train_df.target.values.tolist()\n","    X_valid = valid_df[tweet_col].values.tolist()\n","    y_valid = valid_df[\"target\"].values.tolist()\n","    ds_train = TransformerTweetDataset(X_train, y_train, tokenizer)\n","    ds_valid = TransformerTweetDataset(X_valid, y_valid, tokenizer)    \n","    return ds_train, ds_valid"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.700499Z","iopub.status.busy":"2022-01-18T06:07:55.699997Z","iopub.status.idle":"2022-01-18T06:07:55.710460Z","shell.execute_reply":"2022-01-18T06:07:55.709742Z","shell.execute_reply.started":"2022-01-18T06:07:55.700404Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce179e9007f5482ebba601cc460ae47c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6090 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7772a24a497b4d948386f8547e0ed4db","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1523 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ds_train, ds_val = get_fold_data_hf(0, df_train)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","    num_rows: 6090\n","})"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["ds_train"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.725191Z","iopub.status.busy":"2022-01-18T06:07:55.724862Z","iopub.status.idle":"2022-01-18T06:07:55.730766Z","shell.execute_reply":"2022-01-18T06:07:55.729850Z","shell.execute_reply.started":"2022-01-18T06:07:55.725150Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  return {\n","      'accuracy': acc,\n","  }"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.732721Z","iopub.status.busy":"2022-01-18T06:07:55.732152Z","iopub.status.idle":"2022-01-18T06:07:55.746087Z","shell.execute_reply":"2022-01-18T06:07:55.745239Z","shell.execute_reply.started":"2022-01-18T06:07:55.732685Z"},"trusted":true},"outputs":[],"source":["def get_fold_training_args(fold):\n","    fold_out_dir = f\"./fold_{fold}_results\"\n","    return TrainingArguments(\n","        output_dir=fold_out_dir,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy='epoch',\n","        warmup_ratio=0.1,\n","        num_train_epochs=Config.NUM_EPOCHS,\n","        per_device_train_batch_size=Config.BATCH_SIZE,\n","        per_device_eval_batch_size=Config.BATCH_SIZE,\n","        weight_decay=0.01,\n","        learning_rate=2e-5,    \n","        gradient_accumulation_steps=8    \n","    )"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:07:55.747848Z","iopub.status.busy":"2022-01-18T06:07:55.747226Z","iopub.status.idle":"2022-01-18T06:17:30.130549Z","shell.execute_reply":"2022-01-18T06:17:30.128585Z","shell.execute_reply.started":"2022-01-18T06:07:55.747804Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running training for fold0\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 6090\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 188\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 05:05, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.424999</td>\n","      <td>0.818122</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.384461</td>\n","      <td>0.840446</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.398279</td>\n","      <td>0.833224</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.410809</td>\n","      <td>0.837820</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_0_results/checkpoint-47\n","Configuration saved in ./fold_0_results/checkpoint-47/config.json\n","Model weights saved in ./fold_0_results/checkpoint-47/pytorch_model.bin\n","tokenizer config file saved in ./fold_0_results/checkpoint-47/tokenizer_config.json\n","Special tokens file saved in ./fold_0_results/checkpoint-47/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_0_results/checkpoint-94\n","Configuration saved in ./fold_0_results/checkpoint-94/config.json\n","Model weights saved in ./fold_0_results/checkpoint-94/pytorch_model.bin\n","tokenizer config file saved in ./fold_0_results/checkpoint-94/tokenizer_config.json\n","Special tokens file saved in ./fold_0_results/checkpoint-94/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_0_results/checkpoint-141\n","Configuration saved in ./fold_0_results/checkpoint-141/config.json\n","Model weights saved in ./fold_0_results/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in ./fold_0_results/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in ./fold_0_results/checkpoint-141/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_0_results/checkpoint-188\n","Configuration saved in ./fold_0_results/checkpoint-188/config.json\n","Model weights saved in ./fold_0_results/checkpoint-188/pytorch_model.bin\n","tokenizer config file saved in ./fold_0_results/checkpoint-188/tokenizer_config.json\n","Special tokens file saved in ./fold_0_results/checkpoint-188/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["Running training for fold1\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/bk_anupam/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/bk_anupam/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 6090\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 188\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 05:02, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.428781</td>\n","      <td>0.824688</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.388420</td>\n","      <td>0.840446</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.378548</td>\n","      <td>0.853578</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.392753</td>\n","      <td>0.848326</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_1_results/checkpoint-47\n","Configuration saved in ./fold_1_results/checkpoint-47/config.json\n","Model weights saved in ./fold_1_results/checkpoint-47/pytorch_model.bin\n","tokenizer config file saved in ./fold_1_results/checkpoint-47/tokenizer_config.json\n","Special tokens file saved in ./fold_1_results/checkpoint-47/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_1_results/checkpoint-94\n","Configuration saved in ./fold_1_results/checkpoint-94/config.json\n","Model weights saved in ./fold_1_results/checkpoint-94/pytorch_model.bin\n","tokenizer config file saved in ./fold_1_results/checkpoint-94/tokenizer_config.json\n","Special tokens file saved in ./fold_1_results/checkpoint-94/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_1_results/checkpoint-141\n","Configuration saved in ./fold_1_results/checkpoint-141/config.json\n","Model weights saved in ./fold_1_results/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in ./fold_1_results/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in ./fold_1_results/checkpoint-141/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_1_results/checkpoint-188\n","Configuration saved in ./fold_1_results/checkpoint-188/config.json\n","Model weights saved in ./fold_1_results/checkpoint-188/pytorch_model.bin\n","tokenizer config file saved in ./fold_1_results/checkpoint-188/tokenizer_config.json\n","Special tokens file saved in ./fold_1_results/checkpoint-188/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["Running training for fold2\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/bk_anupam/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/bk_anupam/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 6090\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 188\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [188/188 05:07, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.455982</td>\n","      <td>0.805647</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.417909</td>\n","      <td>0.821405</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.423304</td>\n","      <td>0.828628</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.424576</td>\n","      <td>0.827971</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_2_results/checkpoint-47\n","Configuration saved in ./fold_2_results/checkpoint-47/config.json\n","Model weights saved in ./fold_2_results/checkpoint-47/pytorch_model.bin\n","tokenizer config file saved in ./fold_2_results/checkpoint-47/tokenizer_config.json\n","Special tokens file saved in ./fold_2_results/checkpoint-47/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_2_results/checkpoint-94\n","Configuration saved in ./fold_2_results/checkpoint-94/config.json\n","Model weights saved in ./fold_2_results/checkpoint-94/pytorch_model.bin\n","tokenizer config file saved in ./fold_2_results/checkpoint-94/tokenizer_config.json\n","Special tokens file saved in ./fold_2_results/checkpoint-94/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_2_results/checkpoint-141\n","Configuration saved in ./fold_2_results/checkpoint-141/config.json\n","Model weights saved in ./fold_2_results/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in ./fold_2_results/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in ./fold_2_results/checkpoint-141/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1523\n","  Batch size = 16\n","Saving model checkpoint to ./fold_2_results/checkpoint-188\n","Configuration saved in ./fold_2_results/checkpoint-188/config.json\n","Model weights saved in ./fold_2_results/checkpoint-188/pytorch_model.bin\n","tokenizer config file saved in ./fold_2_results/checkpoint-188/tokenizer_config.json\n","Special tokens file saved in ./fold_2_results/checkpoint-188/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["Running training for fold3\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/bk_anupam/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/bk_anupam/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running training *****\n","  Num examples = 6091\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 8\n","  Total optimization steps = 188\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='153' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [153/188 04:02 < 00:56, 0.62 it/s, Epoch 3.23/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>0.438206</td>\n","      <td>0.818660</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.395755</td>\n","      <td>0.831800</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.402718</td>\n","      <td>0.827201</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1522\n","  Batch size = 16\n","Saving model checkpoint to ./fold_3_results/checkpoint-47\n","Configuration saved in ./fold_3_results/checkpoint-47/config.json\n","Model weights saved in ./fold_3_results/checkpoint-47/pytorch_model.bin\n","tokenizer config file saved in ./fold_3_results/checkpoint-47/tokenizer_config.json\n","Special tokens file saved in ./fold_3_results/checkpoint-47/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1522\n","  Batch size = 16\n","Saving model checkpoint to ./fold_3_results/checkpoint-94\n","Configuration saved in ./fold_3_results/checkpoint-94/config.json\n","Model weights saved in ./fold_3_results/checkpoint-94/pytorch_model.bin\n","tokenizer config file saved in ./fold_3_results/checkpoint-94/tokenizer_config.json\n","Special tokens file saved in ./fold_3_results/checkpoint-94/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 1522\n","  Batch size = 16\n","Saving model checkpoint to ./fold_3_results/checkpoint-141\n","Configuration saved in ./fold_3_results/checkpoint-141/config.json\n","Model weights saved in ./fold_3_results/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in ./fold_3_results/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in ./fold_3_results/checkpoint-141/special_tokens_map.json\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_899/450227553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 )\n\u001b[1;32m    582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     ):\n\u001b[0;32m--> 400\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     ):\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for fold in range(Config.NUM_FOLDS):\n","    fold_str = f\"fold{fold}\"\n","    print(f\"Running training for {fold_str}\")\n","    ds_train, ds_val = get_fold_data(fold, df_train, tokenizer, tweet_col=Config.TWEET_COL)\n","    model = BertForSequenceClassification.from_pretrained(\n","                Config.TRANSFORMER_CHECKPOINT, \n","                num_labels=Config.OUT_SIZE\n","            )\n","    trainer = Trainer(\n","        model=model,                         # the instantiated Transformers model to be trained\n","        args=get_fold_training_args(fold),   # training arguments, defined above\n","        train_dataset=ds_train,              # training dataset\n","        eval_dataset=ds_val,                 # evaluation dataset\n","        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","    trainer.train()\n","    del trainer, model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:00:15.543031Z","iopub.status.busy":"2022-01-18T06:00:15.535907Z","iopub.status.idle":"2022-01-18T06:00:15.561735Z","shell.execute_reply":"2022-01-18T06:00:15.560959Z","shell.execute_reply.started":"2022-01-18T06:00:15.542983Z"},"trusted":true},"outputs":[],"source":["ds_test = TransformerTweetDataset(df_test[Config.TWEET_COL].values.tolist(), None, tokenizer, with_labels=False)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["best_model = BertForSequenceClassification.from_pretrained(\"./fold_1_results/checkpoint-141\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'keyword', 'location', 'text', 'processed_text', 'prcsd_tweet_len'],\n","    num_rows: 3263\n","})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset\n","\n","test_dataset = Dataset.from_pandas(df_test)\n","test_dataset"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def tokenize_tweets(row):\n","    return tokenizer(row[Config.TWEET_COL], truncation=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["test_inputs = tokenizer(df_test[Config.TWEET_COL].values.tolist(), padding=\"longest\", truncation=True, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2d1bb05a1294d44ae86e4b11c45fb32","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_test_dataset = test_dataset.map(tokenize_tweets, batched=True)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['attention_mask', 'id', 'input_ids', 'keyword', 'location', 'prcsd_tweet_len', 'processed_text', 'text', 'token_type_ids'],\n","    num_rows: 3263\n","})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_test_dataset"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:00:15.569654Z","iopub.status.busy":"2022-01-18T06:00:15.569083Z","iopub.status.idle":"2022-01-18T06:00:26.392241Z","shell.execute_reply":"2022-01-18T06:00:26.391430Z","shell.execute_reply.started":"2022-01-18T06:00:15.569620Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","***** Running Prediction *****\n","  Num examples = 3263\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='408' max='408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [408/408 00:38]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["test_args = TrainingArguments(\n","        output_dir=\"./predict\",\n","        do_train=False,\n","        do_eval=False,\n","        do_predict=True\n","    )\n","\n","test_trainer = Trainer(\n","    model=best_model,    \n","    args=test_args,\n","    eval_dataset=ds_test,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer\n",")\n","\n","test_preds = test_trainer.predict(ds_test)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:00:26.393939Z","iopub.status.busy":"2022-01-18T06:00:26.393650Z","iopub.status.idle":"2022-01-18T06:00:26.397993Z","shell.execute_reply":"2022-01-18T06:00:26.397158Z","shell.execute_reply.started":"2022-01-18T06:00:26.393902Z"},"trusted":true},"outputs":[],"source":["test_pred_labels = np.argmax(test_preds.predictions, axis=1)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-01-18T06:00:26.399759Z","iopub.status.busy":"2022-01-18T06:00:26.399307Z","iopub.status.idle":"2022-01-18T06:00:26.421608Z","shell.execute_reply":"2022-01-18T06:00:26.420828Z","shell.execute_reply.started":"2022-01-18T06:00:26.399721Z"},"trusted":true},"outputs":[],"source":["df_submission = pd.read_csv(DATA_PATH + 'submission.csv')\n","df_submission['target']= test_pred_labels\n","df_submission.to_csv('my_submission.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
