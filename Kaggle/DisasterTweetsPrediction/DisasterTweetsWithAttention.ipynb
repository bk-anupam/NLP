{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification with attention (Many to one)\n",
    "We apply an attention layer coupled with encoder decoder architecture for classifying tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits, binary_cross_entropy\n",
    "from torchmetrics import Accuracy, F1\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string\n",
    "import statistics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:\n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 512\n",
    "    EMB_SIZE = 200\n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_EPOCHS = 20\n",
    "    NUM_WORKERS = 8\n",
    "    # Whether to update the pretrained embedding weights during training process\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False    \n",
    "    PATIENCE = 8    \n",
    "    IS_BIDIRECTIONAL = True\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 141, \n",
    "        \"num_layers\": 2,         \n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 7613\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive training examples = 3271\n",
      "No. of negative training examples = 4342\n",
      "No. of unique keywords = 222\n",
      "No of train examples with keyword not null = 7552\n"
     ]
    }
   ],
   "source": [
    "df_train_pos = df_train[df_train.target == 1]\n",
    "df_train_neg = df_train[df_train.target == 0]\n",
    "print(f\"No. of positive training examples = {len(df_train_pos)}\")\n",
    "print(f\"No. of negative training examples = {len(df_train_neg)}\")\n",
    "train_keywords_unique = df_train.keyword.unique()\n",
    "print(f\"No. of unique keywords = {len(train_keywords_unique)}\")\n",
    "df_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\n",
    "print(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[\"target\"].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df        \n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"target\", num_folds=5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct):\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "    return text\n",
    "\n",
    "def process_tweet(df, text, keyword):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n",
    "    processed_text = []\n",
    "    stop = stopwords.words(\"english\")\n",
    "    for tweet, keyword in zip(df[text], df[keyword]):\n",
    "        tweets_clean = []        \n",
    "        # remove stock market tickers like $GE\n",
    "        #tweet = tweet + \" \" + keyword\n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash #, @, ... sign from the word\n",
    "        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n",
    "        tweet = clean_special_chars(tweet, punct)\n",
    "        # remove junk characters which don't have an ascii code\n",
    "        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # tokenize tweets        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            # remove stopwords and punctuation\n",
    "            #if (word.isalpha() and len(word) > 2 and word not in stop \n",
    "            #    and word not in string.punctuation):\n",
    "                #stem_word = stemmer.stem(word)  # stemming word            \n",
    "                #lem_word = lemmatizer.lemmatize(word)\n",
    "                #tweets_clean.append(lem_word) \n",
    "                tweets_clean.append(word)\n",
    "        processed_text.append(\" \".join(tweets_clean))        \n",
    "    df['processed_text'] = np.array(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5174</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Official Website</td>\n",
       "      <td>#HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>hse release annual workplace facility data look</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3126</td>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>aircraft debris found reunion missing malaysia airline</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword          location  \\\n",
       "50  5174  fatalities  Official Website   \n",
       "51  3126      debris               NaN   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "50                  #HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN   \n",
       "51  #??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a   \n",
       "\n",
       "    target  kfold                                          processed_text  \\\n",
       "50       0      4         hse release annual workplace facility data look   \n",
       "51       1      0  aircraft debris found reunion missing malaysia airline   \n",
       "\n",
       "    prcsd_tweet_len  \n",
       "50                7  \n",
       "51                7  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values\n",
    "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n",
    "df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n",
    "process_tweet(df_train, 'text', \"keyword\")\n",
    "process_tweet(df_test, 'text', \"keyword\")\n",
    "# length of the processed tweet\n",
    "df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_train.iloc[50:52, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe word embedding for tweets\n",
    "glove_emb = torchtext.vocab.GloVe(name=\"twitter.27B\", dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tweets vocab from training data\n",
    "def yield_tokens(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield row[\"processed_text\"].split()\n",
    "    \n",
    "tweet_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])   \n",
    "Config.VOCAB_SIZE = len(tweet_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the problem specific vocab, get the embedding vectors from the pre-trained embedding\n",
    "# for each word in vocab and return a matrix of shape vocab_size, embedding_dim. This matrix\n",
    "# will be the pretrained embedding weight matrix which we will use to create the embedding layer\n",
    "def get_vocab_pt_emb_matrix(text_vocab, emb):\n",
    "    embedding_matrix = []\n",
    "    for token in text_vocab.get_itos():\n",
    "        embedding_matrix.append(emb[token])\n",
    "    return torch.stack(embedding_matrix)\n",
    "\n",
    "pt_emb_weights = get_vocab_pt_emb_matrix(tweet_vocab, glove_emb)\n",
    "pt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the processed tweet, i.e. replace each token in the tweet with its corresponding index\n",
    "# in the tweet vocab\n",
    "df_train[\"vectorized_tweet\"] = df_train[\"processed_text\"].apply(\n",
    "    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedTweetDataSet(Dataset):\n",
    "    def __init__(self, tweet_vecs, labels):\n",
    "        self.tweet_vecs = tweet_vecs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_vec = self.tweet_vecs[idx]\n",
    "        label = self.labels[idx]\n",
    "        tweet_len = len(tweet_vec)\n",
    "        return (tweet_vec, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train and validation data for a fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_train = train_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_train = train_df[\"target\"].to_numpy()\n",
    "    X_valid = valid_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_valid = valid_df[\"target\"].to_numpy()\n",
    "    ds_train = VectorizedTweetDataSet(X_train, y_train)\n",
    "    ds_valid = VectorizedTweetDataSet(X_valid, y_valid)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the goal is to train with mini-batches, one needs to pad the sequences in each batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (data, label)\n",
    "    # sort the batch (based on tweet word count) in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # Also need to store the length of each sequence.This is later needed in order to unpad \n",
    "    # the sequences\n",
    "    seq_len = torch.Tensor([len(x) for x in sequences])\n",
    "    labels = torch.Tensor([x[1] for x in sorted_batch])\n",
    "    return sequences_padded, seq_len, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model \n",
    "We use multilayer bidirectional LSTM as the encoder, followed by an attention layer\n",
    "\n",
    "### Bidirectional RNN\n",
    "outputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other\n",
    "\n",
    "hidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class DisasterModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_layers, is_bidirect, emb_size, hidden_size, output_size, \n",
    "                pt_emb_weights, emb_wt_update=False, drop_prob=0.5):\n",
    "        super().__init__()        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers        \n",
    "        # size of the embedding vector\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size   \n",
    "        self.output_dim = output_size\n",
    "        self.is_bidirect = is_bidirect\n",
    "        # Embedding layer\n",
    "        self.emb_layer = nn.Embedding(self.vocab_size, emb_size)\n",
    "        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n",
    "        self.emb_layer.weight.data.copy_(pt_emb_weights)    \n",
    "        # whether to update the pretrained embedding layer weights during model training\n",
    "        self.emb_layer.weight.requires_grad = emb_wt_update            \n",
    "        # LSTM Layer        \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                        input_size=emb_size, \n",
    "                        hidden_size=hidden_size, \n",
    "                        batch_first=True, \n",
    "                        bidirectional=is_bidirect, \n",
    "                        num_layers=num_layers, \n",
    "                        dropout=drop_prob\n",
    "                        )\n",
    "        self.dropout = nn.Dropout(p = drop_prob)                        \n",
    "        \n",
    "        # If the RNN is bidirectional `num_directions` should be 2, else it should be 1.        \n",
    "        if not is_bidirect:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.hidden_size, self.output_dim)\n",
    "        else:       \n",
    "            self.num_directions = 2     \n",
    "            self.linear = nn.Linear(self.hidden_size * self.num_directions, self.output_dim)\n",
    "        # The activation layer which converts output to 0 or 1            \n",
    "        self.act = nn.Sigmoid()            \n",
    "\n",
    "    def forward(self, inputs, input_lengths, state):        \n",
    "        # inputs = [batch_size, batch_max_seq_length]        \n",
    "        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n",
    "        embeds = self.emb_layer(inputs)        \n",
    "        batch_size = inputs.shape[0]        \n",
    "        # embeds = [batch_size, max_seq_length, emb_dim]\n",
    "        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n",
    "        # which elements of the sequence are padded ones and ignore them in computation.\n",
    "        # This step is done only after the embedding step\n",
    "        embeds_pack = pack_padded_sequence(embeds, input_lengths.to(\"cpu\"), batch_first=True)                \n",
    "        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        # unpack the output\n",
    "        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)        \n",
    "        #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "        #print(f\"lstm_out_len.shape = {lstm_out_len.shape}\")\n",
    "        # lstm_out = [batch_size, max_seq_length, hidden_size * num_directions]\n",
    "        if self.is_bidirect:            \n",
    "            # each batch item has different seq length, so to select the hidden state at t_end for each batch item\n",
    "            # a for comprehension like below is needed, a vectorized operation doesn't seem plausible\n",
    "            #lstm_out = [lstm_out[batch_item_index, seq_length_index-1, :] for batch_item_index, seq_length_index in enumerate(lstm_out_len)]            \n",
    "            #lstm_out = torch.cat(lstm_out, dim=0).reshape(batch_size, 4 * self.hidden_size)\n",
    "            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "            # Another way to extract the last hidden state for the forward and backward lstm layers\n",
    "            # in a BiRNN is to use h_n like this\n",
    "            h_tend_fwd = h_n[-2, :, :]\n",
    "            h_tend_bwd = h_n[-1, :, :]\n",
    "            lstm_out = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)\n",
    "            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "        else:                        \n",
    "            lstm_out = h_n[-1, :, :]                    \n",
    "        \n",
    "        out = self.dropout(lstm_out)                \n",
    "        output = self.linear(out)        \n",
    "        # apply sigmoid activation to convert output to probability \n",
    "        output = self.act(output)\n",
    "        # [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "    def init_state(self, batch_size=1):\n",
    "        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n",
    "        to zero\"\"\"\n",
    "        if not isinstance(self.lstm_layer, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "            # hidden state for each element in the batch, c0 = initial cell state\n",
    "            # for each element in the batch\n",
    "            return (torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n",
    "                    torch.zeros((self.num_directions * self.num_layers,batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch lightning wrapper for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetLitModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, emb_size, output_size, pt_emb_weights, emb_wt_update, \n",
    "                hparams, model_eval_metric=MODEL_EVAL_METRIC.accuracy):\n",
    "        super().__init__()\n",
    "        #self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        self.model_eval_metric = model_eval_metric\n",
    "        self.network = DisasterModel(\n",
    "            vocab_size = vocab_size,\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            is_bidirect = Config.IS_BIDIRECTIONAL,\n",
    "            emb_size = emb_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            output_size = output_size,\n",
    "            pt_emb_weights = pt_emb_weights,\n",
    "            emb_wt_update = emb_wt_update,\n",
    "            drop_prob = hparams[\"drop_out\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, tweets, tweet_lengths, state):\n",
    "        return self.network(tweets, tweet_lengths, state)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))        \n",
    "        #print(f\"targets_pred.shape = {targets_pred.shape}\")\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        train_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        train_metric = None\n",
    "        train_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)            \n",
    "            train_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            train_metric_str = \"train_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            train_metric = F1(targets_pred, targets)            \n",
    "            train_metric_str = \"train_f1\"\n",
    "        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        val_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        val_metric = None\n",
    "        val_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:\n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)\n",
    "            val_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            val_metric_str = \"val_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            val_metric = F1(targets_pred, targets)            \n",
    "            val_metric_str = \"val_f1\"\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom lightning callback \n",
    "To record training and validation metric values at each epoch and the best metric values across all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "# Monitor multiple metric values that are calculated either in training or validation step and return the\n",
    "# best metric values for each epoch\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n",
    "        # dictionary with metric name as key and monitor mode (min, max) as the value\n",
    "        # ( the same names used to log metric values in training and validation step)\n",
    "        self.val_metrics_to_monitor = val_metrics_to_monitor\n",
    "        self.train_metrics_to_monitor = train_metrics_to_monitor\n",
    "        # dictionary with metric_name as key and list of metric value for each epoch\n",
    "        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the best metric value for all epochs\n",
    "        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the epoch number with the best metric value\n",
    "        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n",
    "        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n",
    "        self.epoch_counter = 0           \n",
    "\n",
    "    @staticmethod\n",
    "    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n",
    "        metric_str = \"\"\n",
    "        for metric, mode in metrics_to_monitor.items():\n",
    "            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n",
    "            metric_str += f\"{metric} = {metric_value}, \"\n",
    "            metrics[metric].append(metric_value)\n",
    "            if mode == \"max\":\n",
    "                best_metric[metric] = max(metrics[metric])            \n",
    "            elif mode == \"min\":            \n",
    "                best_metric[metric] = min(metrics[metric])            \n",
    "            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n",
    "        print(metric_str[:-2])\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.epoch_counter += 1        \n",
    "        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n",
    "        print(f\"For epoch {self.epoch_counter}\")\n",
    "        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")\n",
    "    disaster_tweet_model = DisasterTweetLitModel(\n",
    "        vocab_size=Config.VOCAB_SIZE,\n",
    "        emb_size=Config.EMB_SIZE,\n",
    "        output_size=Config.OUT_SIZE,\n",
    "        pt_emb_weights=pt_emb_weights,\n",
    "        emb_wt_update=Config.EMB_WT_UPDATE,\n",
    "        hparams=Config.MODEL_PARAMS,\n",
    "        model_eval_metric=Config.MODEL_EVAL_METRIC                \n",
    "        )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"\n",
    "    train_metrics_to_monitor = {\n",
    "        \"train_loss\": \"min\",\n",
    "        \"train_acc\": \"max\"\n",
    "    }\n",
    "    val_metrics_to_monitor = {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        }\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus = 1,\n",
    "        deterministic = True,\n",
    "        auto_select_gpus = True,\n",
    "        progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        auto_lr_find = True,    \n",
    "        #precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        gradient_clip_val = 1.0,        \n",
    "        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n",
    "        print(disaster_tweet_model.lr)\n",
    "    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_train_metrics = {\n",
    "        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n",
    "        for metric in train_metrics_to_monitor.keys()\n",
    "    }\n",
    "    fold_val_metrics = {\n",
    "        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n",
    "        for metric in val_metrics_to_monitor.keys()\n",
    "    }            \n",
    "    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n",
    "    return fold_train_metrics, fold_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | network | DisasterModel | 3.8 M \n",
      "------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.122    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48420fbd2b214100a209a6f978d4e4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6969, val_acc = 0.4297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4b441b5e3e45d18a17ecdd4104849e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360c17f171aa46768135462d3a44cdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.661\n",
      "Epoch 0, global step 11: val_loss reached 0.66065 (best 0.66065), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=0_val_loss=0.6607.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6607, val_acc = 0.5798\n",
      "train_loss = 0.6817, train_acc = 0.556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc0afc3ac254ab7af68fb5e1e90f27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.094 >= min_delta = 0.0. New best score: 0.567\n",
      "Epoch 1, global step 23: val_loss reached 0.56666 (best 0.56666), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=1_val_loss=0.5667.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.5667, val_acc = 0.761\n",
      "train_loss = 0.6252, train_acc = 0.6516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844509b22fea46bd8e8fb082f3bf378f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.510\n",
      "Epoch 2, global step 35: val_loss reached 0.50960 (best 0.50960), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=2_val_loss=0.5096.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.5096, val_acc = 0.759\n",
      "train_loss = 0.5243, train_acc = 0.7639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3151c08b364b49378aa1c160aaa01009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.455\n",
      "Epoch 3, global step 47: val_loss reached 0.45512 (best 0.45512), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=3_val_loss=0.4551.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.4551, val_acc = 0.7997\n",
      "train_loss = 0.4627, train_acc = 0.7941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab449d2ae1634bbd9a82c00d575159c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.437\n",
      "Epoch 4, global step 59: val_loss reached 0.43668 (best 0.43668), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=4_val_loss=0.4367.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.4367, val_acc = 0.8148\n",
      "train_loss = 0.4251, train_acc = 0.8167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4b615e01df4b87bd00ffe3e56ef7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.436\n",
      "Epoch 5, global step 71: val_loss reached 0.43600 (best 0.43600), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/best_model_epoch=5_val_loss=0.4360.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.436, val_acc = 0.8096\n",
      "train_loss = 0.3988, train_acc = 0.8317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919f41e65ba419386b9534aae41dc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 83: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.4385, val_acc = 0.8102\n",
      "train_loss = 0.3744, train_acc = 0.8473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdc42ce64b74404882670d12c668153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 95: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.4522, val_acc = 0.8037\n",
      "train_loss = 0.3506, train_acc = 0.8567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efc4096dc984a69962cc3c469d244c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 107: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.4521, val_acc = 0.8122\n",
      "train_loss = 0.3282, train_acc = 0.8688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3af4547bd404d2bbb86333ad3454206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 119: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.474, val_acc = 0.807\n",
      "train_loss = 0.3026, train_acc = 0.8819\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7abbe8eb3457eb6699806f2a1e168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 131: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10\n",
      "val_loss = 0.5265, val_acc = 0.7965\n",
      "train_loss = 0.277, train_acc = 0.8923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0a614df17b4c539d56b7d0ba2cad17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11\n",
      "val_loss = 0.566, val_acc = 0.7873\n",
      "train_loss = 0.2499, train_acc = 0.9072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0687f3a325844753b655a184de9ff6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 155: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12\n",
      "val_loss = 0.5606, val_acc = 0.8037\n",
      "train_loss = 0.2226, train_acc = 0.9204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12e60145ca846e7bdf37cc1567ff891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13\n",
      "val_loss = 0.66, val_acc = 0.78\n",
      "train_loss = 0.1961, train_acc = 0.9332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f304101ca2942439ff37a49d8895cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 179: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14\n",
      "val_loss = 0.6841, val_acc = 0.7912\n",
      "train_loss = 0.1722, train_acc = 0.9404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1377047e42414d5384dc67be50732624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.436. Signaling Trainer to stop.\n",
      "Epoch 15, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15\n",
      "val_loss = 0.7264, val_acc = 0.7748\n",
      "train_loss = 0.1509, train_acc = 0.9494\n",
      "Best train metrics values for fold0\n",
      "{'train_loss': (0.1509, 15), 'train_acc': (0.9494, 15)}\n",
      "Best val metrics values for fold0\n",
      "{'val_loss': (0.436, 6), 'val_acc': (0.8148, 5)}\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "all_fold_val_loss = []\n",
    "all_fold_val_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val = get_fold_dls(fold, df_train)\n",
    "    fold_train_metrics, fold_val_metrics = run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=False)    \n",
    "    all_fold_val_loss.append(fold_val_metrics[\"val_loss\"])\n",
    "    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"])\n",
    "    print(f\"Best train metrics values for fold{fold}\")    \n",
    "    print(fold_train_metrics)\n",
    "    print(f\"Best val metrics values for fold{fold}\")    \n",
    "    print(fold_val_metrics)        \n",
    "\n",
    "mean_loss = statistics.mean(all_fold_val_loss)\n",
    "mean_acc = statistics.mean(all_fold_val_acc)\n",
    "std_loss = statistics.stdev(all_fold_val_loss)\n",
    "std_acc = statistics.stddev(all_fold_val_acc)\n",
    "print(f\"val loss across folds = {all_fold_val_loss}\")\n",
    "print(f\"mean val loss across folds = {mean_loss}, val loss stdev across fold = {std_loss}\")\n",
    "print(f\"val accuracy across folds = {all_fold_val_acc}\")\n",
    "print(f\"mean val accuracy across folds = {mean_acc}, val accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = torch.load(\"best_model.pt\")\n",
    "# print(best_model)\n",
    "# tweet_vocab.set_default_index(0)\n",
    "# df_test[\"vectorized_tweet\"] = df_test[\"processed_text\"].apply(\n",
    "#     lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "#     )\n",
    "\n",
    "# # Do prediction with best performing model on the test set\n",
    "# def predict(df_test):\n",
    "#     test_output = []\n",
    "#     for index, row in df_test.iterrows():    \n",
    "#         vec_tweet = row[\"vectorized_tweet\"]\n",
    "#         if len(vec_tweet) == 0:\n",
    "#             test_output.append(0)\n",
    "#             continue\n",
    "#         vec_tweet_len = torch.IntTensor([len(vec_tweet)])\n",
    "#         vec_tweet = vec_tweet.view(1, -1)    \n",
    "#         #print(vec_tweet, vec_tweet_len)\n",
    "#         output, (h_n,c_n) = best_model(vec_tweet, vec_tweet_len, state=None)\n",
    "#         #print(output)\n",
    "#         test_output.append(round(output.item()))    \n",
    "#     return test_output        \n",
    "\n",
    "# test_output = predict(df_test)\n",
    "# print(len(test_output))\n",
    "\n",
    "# df_submission = pd.read_csv('./data/submission.csv')\n",
    "# df_submission['target']= test_output\n",
    "# df_submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
