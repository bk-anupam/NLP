{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchmetrics import Accuracy, F1\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:\n",
    "    TWEET_COL = \"text\"\n",
    "    RANDOM_STATE = 42\n",
    "    BATCH_SIZE = 16\n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 3\n",
    "    NUM_EPOCHS = 3\n",
    "    NUM_WORKERS = 8\n",
    "    TRANSFORMER_CHECKPOINT = \"bert-base-uncased\"\n",
    "    # The hidden_size of the output of the last layer of the transformer model used\n",
    "    TRANSFORMER_OUT_SIZE = 768\n",
    "    PAD_TOKEN_ID = 0\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False    \n",
    "    PATIENCE = 5    \n",
    "    IS_BIDIRECTIONAL = True\n",
    "    # model hyperparameters\n",
    "    MODEL_HPARAMS = {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"warmup_steps\": 0\n",
    "    }\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_STATE, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 7613\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold CV\n",
    "Split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "on the data. We use stratified kfold if the target distribution is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold_dataframe(df, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df.sample(frac=1, random_state=Config.RANDOM_STATE).reset_index(drop=True)\n",
    "    y = df[\"target\"].values\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "    # stratification is done on the basis of y labels, a placeholder for X is sufficient\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_idx, \"kfold\"] = fold\n",
    "    return df\n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, num_folds=Config.NUM_FOLDS)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct):\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "    return text\n",
    "\n",
    "def process_tweet(df, text, keyword):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n",
    "    processed_text = []\n",
    "    stop = stopwords.words(\"english\")\n",
    "    for tweet, keyword in zip(df[text], df[keyword]):\n",
    "        tweets_clean = []        \n",
    "        # remove stock market tickers like $GE        \n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash #, @, ... sign from the word\n",
    "        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n",
    "        tweet = clean_special_chars(tweet, punct)\n",
    "        # remove junk characters which don't have an ascii code\n",
    "        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # tokenize tweets        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            # remove stopwords and punctuation\n",
    "            #if (word.isalpha() and len(word) > 2 and word not in stop and word not in string.punctuation):\n",
    "                #stem_word = stemmer.stem(word)  # stemming word            \n",
    "                #lem_word = lemmatizer.lemmatize(word)\n",
    "                #tweets_clean.append(lem_word) \n",
    "                tweets_clean.append(word)\n",
    "        processed_text.append(\" \".join(tweets_clean))        \n",
    "    df['processed_text'] = np.array(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>73</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Sheffield Township, Ohio</td>\n",
       "      <td>Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>deputies man shot before brighton home set ablaze</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>74</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>India</td>\n",
       "      <td>Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>man wife get six years jail for setting ablaze niece</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                  location  \\\n",
       "50  73  ablaze  Sheffield Township, Ohio   \n",
       "51  74  ablaze                     India   \n",
       "\n",
       "                                                                            text  \\\n",
       "50     Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k   \n",
       "51  Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA   \n",
       "\n",
       "    target  kfold                                        processed_text  \\\n",
       "50       1      1     deputies man shot before brighton home set ablaze   \n",
       "51       1      0  man wife get six years jail for setting ablaze niece   \n",
       "\n",
       "    prcsd_tweet_len  \n",
       "50                8  \n",
       "51               10  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values\n",
    "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n",
    "df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n",
    "process_tweet(df_train, 'text', \"keyword\")\n",
    "process_tweet(df_test, 'text', \"keyword\")\n",
    "# length of the processed tweet\n",
    "df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_train.iloc[50:52, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for transformer model\n",
    "Converts tweets into pytorch dataset compatible with BERT and other transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweets to data that the BERT model understands\n",
    "class TransformerTweetDataset(Dataset):\n",
    "    def __init__(self, tweets, targets):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(Config.TRANSFORMER_CHECKPOINT)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, item_idx):\n",
    "        inputs = self.tokenizer(self.tweets[item_idx])\n",
    "        targets = self.targets[item_idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.LongTensor(inputs[\"input_ids\"]),\n",
    "            \"token_type_ids\": torch.LongTensor(inputs[\"token_type_ids\"]),\n",
    "            \"attention_mask\": torch.LongTensor(inputs[\"attention_mask\"]),\n",
    "            \"targets\": targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_seq(batch, key, padding_value):    \n",
    "    seq = [dict_item[key] for dict_item in batch]    \n",
    "    seq_sorted = sorted(seq, key=lambda x:x.shape[0], reverse=True)    \n",
    "    return pad_sequence(seq_sorted, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "def pad_collate(batch):        \n",
    "    # batch is a list of dictionaries where each each dictionary is one data row    \n",
    "    input_ids_padded = pad_input_seq(batch, \"input_ids\", Config.PAD_TOKEN_ID)\n",
    "    token_type_ids_padded = pad_input_seq(batch, \"token_type_ids\", Config.PAD_TOKEN_ID)\n",
    "    attention_mask_padded = pad_input_seq(batch, \"attention_mask\", 0)\n",
    "    batch_targets = torch.LongTensor([dict_item[\"targets\"] for dict_item in batch])\n",
    "    batch_padded = {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"token_type_ids\": token_type_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": batch_targets\n",
    "    } \n",
    "    return batch_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train and validation data for a fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df, tweet_col=\"text\"):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_train = train_df[tweet_col].values.tolist()\n",
    "    y_train = train_df.target.values.tolist()\n",
    "    X_valid = valid_df[tweet_col].values.tolist()\n",
    "    y_valid = valid_df[\"target\"].values.tolist()\n",
    "    ds_train = TransformerTweetDataset(X_train, y_train)\n",
    "    ds_valid = TransformerTweetDataset(X_valid, y_valid)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_valid, ds_train, ds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid, ds_train, ds_valid = get_fold_dls(0, df_train, Config.TWEET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dl = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_iter = iter(my_dl)\n",
    "# item = next(dl_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = item[\"targets\"]\n",
    "# print(f\"targets.shape={targets.shape}\")\n",
    "# labels = F.one_hot(targets.T.long(), num_classes=2)\n",
    "# #labels = labels.float()        \n",
    "# print(f\"labels.shape = {labels.shape}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_tmodel = AutoModelForSequenceClassification.from_pretrained(Config.TRANSFORMER_CHECKPOINT, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = my_tmodel(\n",
    "#     input_ids = item[\"input_ids\"], \n",
    "#     token_type_ids = item[\"token_type_ids\"], \n",
    "#     attention_mask = item[\"attention_mask\"], \n",
    "#     labels = targets.long()\n",
    "#     )\n",
    "# print(output.loss)\n",
    "# print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer model for tweet classification using pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class TransformerTweetLitModel(pl.LightningModule):\n",
    "    def __init__(self, params, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = hparams[\"learning_rate\"]     \n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        self.adam_epsilon = hparams[\"adam_epsilon\"]\n",
    "        self.warmup_steps = hparams[\"warmup_steps\"]   \n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            Config.TRANSFORMER_CHECKPOINT, \n",
    "            num_labels=Config.OUT_SIZE\n",
    "            )\n",
    "        self.num_train_steps = params[\"num_train_steps\"]\n",
    "        self.model_eval_metric = params[\"model_eval_metric\"]\n",
    "        print(self.hparams)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels):\n",
    "        return self.transformer_model(\n",
    "            input_ids = input_ids,\n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.transformer_model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.num_train_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        # input_ids = batch[\"input_ids\"]\n",
    "        # token_type_ids = batch[\"token_type_ids\"]\n",
    "        # attention_mask = batch[\"attention_mask\"]\n",
    "        # targets = batch[\"targets\"]\n",
    "        output = self(**batch)     \n",
    "        labels = batch[\"labels\"]   \n",
    "        train_metric = None\n",
    "        train_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:    \n",
    "            targets_pred = torch.argmax(output.logits, dim=1)            \n",
    "            train_metric = Accuracy(num_classes=2)(targets_pred.cpu(), labels.cpu())\n",
    "            train_metric_str = \"train_acc\"\n",
    "        # elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "        #     train_metric = F1(output.logits, targets.float())            \n",
    "        #     train_metric_str = \"train_f1\"\n",
    "        self.log(\"train_loss\", output.loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return output.loss                    \n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     input_ids = batch[\"input_ids\"]\n",
    "    #     token_type_ids = batch[\"token_type_ids\"]\n",
    "    #     attention_mask = batch[\"attention_mask\"]\n",
    "    #     targets = batch[\"targets\"]                \n",
    "    #     output = self(input_ids, token_type_ids, attention_mask, targets)\n",
    "    #     val_metric = None\n",
    "    #     val_metric_str = \"\"\n",
    "    #     if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n",
    "    #         targets_pred = torch.argmax(output.logits, dim=1)            \n",
    "    #         val_metric = Accuracy(num_classes=2)(targets_pred.detach().cpu(), targets.detach().cpu())\n",
    "    #         val_metric_str = \"val_acc\"\n",
    "    #     # elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "    #     #     val_metric = F1(output.logits, targets.float())            \n",
    "    #     #     val_metric_str = \"val_f1\"\n",
    "    #     self.log(\"val_loss\", output.loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "    #     self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "    #     return output.loss                \n",
    "     \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        labels = batch[\"labels\"]\n",
    "        val_metric = None\n",
    "        val_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n",
    "            targets_pred = torch.argmax(logits, dim=1)            \n",
    "            val_metric = Accuracy(num_classes=2)(targets_pred.detach().cpu(), labels.detach().cpu())\n",
    "            val_metric_str = \"val_acc\"\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}   \n",
    "\n",
    "    # def validation_epoch_end(self, outputs):        \n",
    "    #     preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "    #     labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "    #     loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    #     self.log(\"val_loss\", loss, prog_bar=True)\n",
    "    #     self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "    #     return loss         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom lightning callback \n",
    "To record training and validation metric values at each epoch and the best metric values across all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "# Monitor multiple metric values that are calculated either in training or validation step and return the\n",
    "# best metric values for each epoch\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n",
    "        # dictionary with metric name as key and monitor mode (min, max) as the value\n",
    "        # ( the same names used to log metric values in training and validation step)\n",
    "        self.val_metrics_to_monitor = val_metrics_to_monitor\n",
    "        self.train_metrics_to_monitor = train_metrics_to_monitor\n",
    "        # dictionary with metric_name as key and list of metric value for each epoch\n",
    "        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the best metric value for all epochs\n",
    "        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the epoch number with the best metric value\n",
    "        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n",
    "        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n",
    "        self.epoch_counter = 0           \n",
    "\n",
    "    @staticmethod\n",
    "    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n",
    "        metric_str = \"\"\n",
    "        for metric, mode in metrics_to_monitor.items():\n",
    "            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n",
    "            metric_str += f\"{metric} = {metric_value}, \"\n",
    "            metrics[metric].append(metric_value)\n",
    "            if mode == \"max\":\n",
    "                best_metric[metric] = max(metrics[metric])            \n",
    "            elif mode == \"min\":            \n",
    "                best_metric[metric] = min(metrics[metric])            \n",
    "            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n",
    "        print(metric_str[:-2])\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.epoch_counter += 1        \n",
    "        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n",
    "        print(f\"For epoch {self.epoch_counter}\")\n",
    "        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_train_steps1 = (len(df_train) / Config.BATCH_SIZE) * Config.NUM_EPOCHS\n",
    "# num_train_steps2 = len(dl_train) * Config.NUM_EPOCHS\n",
    "# num_val_steps = len(dl_valid) * Config.NUM_EPOCHS\n",
    "# print(f\"num_train_steps1 = {num_train_steps1}\")\n",
    "# print(f\"num_train_steps2 = {num_train_steps2}\")\n",
    "# print(f\"num_val_steps = {num_val_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")\n",
    "    num_train_steps = int(len(dl_train) * Config.NUM_EPOCHS)\n",
    "    print(f\"num_train_steps = {num_train_steps}\")\n",
    "    disaster_tweet_model = TransformerTweetLitModel(                \n",
    "        params = {\n",
    "            \"model_eval_metric\": Config.MODEL_EVAL_METRIC,\n",
    "            \"num_train_steps\": num_train_steps\n",
    "        },\n",
    "        hparams=Config.MODEL_HPARAMS\n",
    "    )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = fold_str + \"_best_model_{epoch}_{val_loss:.4f}\"\n",
    "    train_metrics_to_monitor = {\n",
    "        \"train_loss\": \"min\",\n",
    "        \"train_acc\": \"max\"\n",
    "    }\n",
    "    val_metrics_to_monitor = {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        }\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus = 1,\n",
    "        deterministic = True,\n",
    "        auto_select_gpus = True,\n",
    "        progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        auto_lr_find = True,    \n",
    "        #precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        gradient_clip_val = 1.0,        \n",
    "        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n",
    "        print(disaster_tweet_model.lr)\n",
    "    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_train_metrics = {\n",
    "        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n",
    "        for metric in train_metrics_to_monitor.keys()\n",
    "    }\n",
    "    fold_val_metrics = {\n",
    "        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n",
    "        for metric in val_metrics_to_monitor.keys()\n",
    "    }            \n",
    "    best_model = loss_chkpt_callback.best_model_path\n",
    "    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n",
    "    return fold_train_metrics, fold_val_metrics, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n",
      "num_train_steps = 795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hparams\": {'learning_rate': 2e-05, 'adam_epsilon': 1e-08, 'weight_decay': 0.0, 'warmup_steps': 0}\n",
      "\"params\":  {'model_eval_metric': 'accuracy', 'num_train_steps': 795}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                          | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | transformer_model | BertForSequenceClassification | 109 M \n",
      "--------------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af93bd97205549bb87a8b26326714541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.702, val_acc = 0.4688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aeac3920504ccd88d2092961abca8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13c9cd737a4af8b6edb99cfc20c0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.684\n",
      "Epoch 0, global step 158: val_loss reached 0.68447 (best 0.68447), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=0_val_loss=0.6845.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6845, val_acc = 0.5701\n",
      "train_loss = 0.6907, train_acc = 0.5513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a39aaaae30a46b49043469d08dd0bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.683\n",
      "Epoch 1, global step 317: val_loss reached 0.68336 (best 0.68336), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=1_val_loss=0.6834.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.6834, val_acc = 0.5701\n",
      "train_loss = 0.6858, train_acc = 0.5643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best train metrics values for fold0\n",
      "{'train_loss': (0.6858, 1), 'train_acc': (0.5643, 1)}\n",
      "Best val metrics values for fold0\n",
      "{'val_loss': (0.6834, 2), 'val_acc': (0.5701, 1)}\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "all_fold_val_loss = []\n",
    "all_fold_val_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, df_train, tweet_col=Config.TWEET_COL)\n",
    "    fold_train_metrics, fold_val_metrics, chkpt_file_name = run_training(fold, dl_train, dl_val, find_lr=False)    \n",
    "    all_fold_val_loss.append((fold_val_metrics[\"val_loss\"][0], chkpt_file_name))\n",
    "    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"][0])\n",
    "    print(f\"Best train metrics values for fold{fold}\")    \n",
    "    print(fold_train_metrics)\n",
    "    print(f\"Best val metrics values for fold{fold}\")    \n",
    "    print(fold_val_metrics)     \n",
    "    break   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
