{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:35.989692Z",
     "iopub.status.busy": "2021-09-24T11:29:35.988494Z",
     "iopub.status.idle": "2021-09-24T11:29:37.721542Z",
     "shell.execute_reply": "2021-09-24T11:29:37.720795Z",
     "shell.execute_reply.started": "2021-09-24T11:25:43.524893Z"
    },
    "papermill": {
     "duration": 1.765907,
     "end_time": "2021-09-24T11:29:37.721745",
     "exception": false,
     "start_time": "2021-09-24T11:29:35.955838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 7613\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:37.787808Z",
     "iopub.status.busy": "2021-09-24T11:29:37.787078Z",
     "iopub.status.idle": "2021-09-24T11:29:37.822364Z",
     "shell.execute_reply": "2021-09-24T11:29:37.821464Z",
     "shell.execute_reply.started": "2021-09-24T11:25:45.378728Z"
    },
    "papermill": {
     "duration": 0.071705,
     "end_time": "2021-09-24T11:29:37.822571",
     "exception": false,
     "start_time": "2021-09-24T11:29:37.750866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive training examples = 3271\n",
      "No. of negative training examples = 4342\n",
      "No. of unique keywords = 222\n",
      "No of train examples with keyword not null = 7552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "\n",
       "                                                                                  text  \\\n",
       "31                             @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C   \n",
       "32                 We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw   \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi   \n",
       "34                                                  Crying out for more! Set me ablaze   \n",
       "35        On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N   \n",
       "\n",
       "    target  \n",
       "31       1  \n",
       "32       0  \n",
       "33       1  \n",
       "34       0  \n",
       "35       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pos = df_train[df_train.target == 1]\n",
    "df_train_neg = df_train[df_train.target == 0]\n",
    "print(f\"No. of positive training examples = {len(df_train_pos)}\")\n",
    "print(f\"No. of negative training examples = {len(df_train_neg)}\")\n",
    "train_keywords_unique = df_train.keyword.unique()\n",
    "print(f\"No. of unique keywords = {len(train_keywords_unique)}\")\n",
    "df_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\n",
    "print(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")\n",
    "df_train_notnull_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:37.892195Z",
     "iopub.status.busy": "2021-09-24T11:29:37.891301Z",
     "iopub.status.idle": "2021-09-24T11:29:37.894938Z",
     "shell.execute_reply": "2021-09-24T11:29:37.895466Z",
     "shell.execute_reply.started": "2021-09-24T11:25:45.423964Z"
    },
    "papermill": {
     "duration": 0.043933,
     "end_time": "2021-09-24T11:29:37.895658",
     "exception": false,
     "start_time": "2021-09-24T11:29:37.851725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   0     NaN      NaN   \n",
       "1   2     NaN      NaN   \n",
       "2   3     NaN      NaN   \n",
       "3   9     NaN      NaN   \n",
       "4  11     NaN      NaN   \n",
       "\n",
       "                                                                                               text  \n",
       "0                                                                Just happened a terrible car crash  \n",
       "1                                  Heard about #earthquake is different cities, stay safe everyone.  \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3                                                          Apocalypse lighting. #Spokane #wildfires  \n",
       "4                                                     Typhoon Soudelor kills 28 in China and Taiwan  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[\"target\"].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df        \n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"target\", num_folds=5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029155,
     "end_time": "2021-09-24T11:29:37.954056",
     "exception": false,
     "start_time": "2021-09-24T11:29:37.924901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocess the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:43.429967Z",
     "iopub.status.busy": "2021-09-24T11:29:43.429217Z",
     "iopub.status.idle": "2021-09-24T11:29:43.432488Z",
     "shell.execute_reply": "2021-09-24T11:29:43.431898Z",
     "shell.execute_reply.started": "2021-09-24T11:25:50.679824Z"
    },
    "papermill": {
     "duration": 0.044876,
     "end_time": "2021-09-24T11:29:43.432656",
     "exception": false,
     "start_time": "2021-09-24T11:29:43.387780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct):\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "    return text\n",
    "\n",
    "def process_tweet(df, text, keyword):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n",
    "    processed_text = []\n",
    "    stop = stopwords.words(\"english\")\n",
    "    for tweet, keyword in zip(df[text], df[keyword]):\n",
    "        tweets_clean = []        \n",
    "        # remove stock market tickers like $GE\n",
    "        #tweet = tweet + \" \" + keyword\n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash #, @, ... sign from the word\n",
    "        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n",
    "        tweet = clean_special_chars(tweet, punct)\n",
    "        # remove junk characters which don't have an ascii code\n",
    "        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # tokenize tweets        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            # remove stopwords and punctuation\n",
    "            #if (word.isalpha() and len(word) > 2 #and word not in stop \n",
    "            #    and word not in string.punctuation):\n",
    "                #stem_word = stemmer.stem(word)  # stemming word            \n",
    "                #lem_word = lemmatizer.lemmatize(word)\n",
    "                #tweets_clean.append(lem_word) \n",
    "                tweets_clean.append(word)\n",
    "        processed_text.append(\" \".join(tweets_clean))        \n",
    "    df['processed_text'] = np.array(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:43.494947Z",
     "iopub.status.busy": "2021-09-24T11:29:43.493949Z",
     "iopub.status.idle": "2021-09-24T11:29:48.131903Z",
     "shell.execute_reply": "2021-09-24T11:29:48.132422Z",
     "shell.execute_reply.started": "2021-09-24T11:25:50.692578Z"
    },
    "papermill": {
     "duration": 4.670808,
     "end_time": "2021-09-24T11:29:48.132640",
     "exception": false,
     "start_time": "2021-09-24T11:29:43.461832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n",
    "df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n",
    "process_tweet(df_train, 'text', \"keyword\")\n",
    "process_tweet(df_test, 'text', \"keyword\")\n",
    "df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.597136477078681"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"prcsd_tweet_len\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:48.195063Z",
     "iopub.status.busy": "2021-09-24T11:29:48.194413Z",
     "iopub.status.idle": "2021-09-24T11:29:48.207920Z",
     "shell.execute_reply": "2021-09-24T11:29:48.208468Z",
     "shell.execute_reply.started": "2021-09-24T11:25:55.413845Z"
    },
    "papermill": {
     "duration": 0.046078,
     "end_time": "2021-09-24T11:29:48.208659",
     "exception": false,
     "start_time": "2021-09-24T11:29:48.162581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5174</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Official Website</td>\n",
       "      <td>#HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>hse releases annual workplace facilities data have a look</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3126</td>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mh370 aircraft debris found on la reunion is from missing malaysia airlines</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword          location  \\\n",
       "50  5174  fatalities  Official Website   \n",
       "51  3126      debris               NaN   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "50                  #HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN   \n",
       "51  #??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a   \n",
       "\n",
       "    target  kfold  \\\n",
       "50       0      4   \n",
       "51       1      0   \n",
       "\n",
       "                                                                 processed_text  \\\n",
       "50                    hse releases annual workplace facilities data have a look   \n",
       "51  mh370 aircraft debris found on la reunion is from missing malaysia airlines   \n",
       "\n",
       "    prcsd_tweet_len  \n",
       "50                9  \n",
       "51               12  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[50:52, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-24T11:29:48.271874Z",
     "iopub.status.busy": "2021-09-24T11:29:48.271196Z",
     "iopub.status.idle": "2021-09-24T11:29:48.285535Z",
     "shell.execute_reply": "2021-09-24T11:29:48.284912Z",
     "shell.execute_reply.started": "2021-09-24T11:25:55.431885Z"
    },
    "papermill": {
     "duration": 0.046942,
     "end_time": "2021-09-24T11:29:48.285685",
     "exception": false,
     "start_time": "2021-09-24T11:29:48.238743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "      <td>heard about earthquake is different cities stay safe everyone</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "      <td>there is a forest fire at spot pond geese are fleeing across the street i cannot save them all</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     keyword location  \\\n",
       "0   0  no_keyword      NaN   \n",
       "1   2  no_keyword      NaN   \n",
       "2   3  no_keyword      NaN   \n",
       "3   9  no_keyword      NaN   \n",
       "4  11  no_keyword      NaN   \n",
       "\n",
       "                                                                                               text  \\\n",
       "0                                                                Just happened a terrible car crash   \n",
       "1                                  Heard about #earthquake is different cities, stay safe everyone.   \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all   \n",
       "3                                                          Apocalypse lighting. #Spokane #wildfires   \n",
       "4                                                     Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                                                                   processed_text  \\\n",
       "0                                                              just happened a terrible car crash   \n",
       "1                                   heard about earthquake is different cities stay safe everyone   \n",
       "2  there is a forest fire at spot pond geese are fleeing across the street i cannot save them all   \n",
       "3                                                           apocalypse lighting spokane wildfires   \n",
       "4                                                   typhoon soudelor kills 28 in china and taiwan   \n",
       "\n",
       "   prcsd_tweet_len  \n",
       "0                6  \n",
       "1                9  \n",
       "2               19  \n",
       "3                4  \n",
       "4                8  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us try some deep learning techniques now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding_dict(embedding_file_path):\n",
    "    embedding_dict = {}\n",
    "    with open(embedding_file_path, \"r\") as f:\n",
    "        # https://stackoverflow.com/questions/8009882/how-to-read-a-large-file-line-by-line\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_vec = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = word_vec\n",
    "    return embedding_dict        \n",
    "\n",
    "#glove_embedding_dict = get_word_embedding_dict(\"../../../glove.twitter.27B/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb = torchtext.vocab.GloVe(name=\"twitter.27B\", dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tweets vocab from training data\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield row[\"processed_text\"].split()\n",
    "    \n",
    "tweet_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b8176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = tweet_vocab.get_stoi()\n",
    "vocab_dict[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the problem specific vocab, get the embedding vectors from the pre-trained embedding\n",
    "# for each word in vocab and return a matrix of shape vocab_size, embedding_dim. This matrix\n",
    "# will be the pretrained embedding weight matrix which we will use to create the embedding layer\n",
    "def get_vocab_pt_emb_matrix(text_vocab, emb):\n",
    "    embedding_matrix = []\n",
    "    for token in text_vocab.get_itos():\n",
    "        embedding_matrix.append(emb[token])\n",
    "    return torch.stack(embedding_matrix)\n",
    "\n",
    "pt_emb_weights = get_vocab_pt_emb_matrix(tweet_vocab, glove_emb)\n",
    "pt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(index, token) for index, token in enumerate(glove_emb.itos) if token == \"<unk>\"]\n",
    "#pt_emb_layer(torch.LongTensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "df_train[\"vectorized_tweet\"] = df_train[\"processed_text\"].apply(\n",
    "    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "    )\n",
    "\n",
    "#x_seq = df_train[\"vectorized_tweet\"].values.tolist()\n",
    "# the index for 'pad' token in tweet_vocab is 1.\n",
    "#x_padded_seq = pad_sequence(x_seq, batch_first=True, padding_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedTweetDataSet(Dataset):\n",
    "    def __init__(self, tweet_vecs, labels):\n",
    "        self.tweet_vecs = tweet_vecs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_vec = self.tweet_vecs[idx]\n",
    "        label = self.labels[idx]\n",
    "        tweet_len = len(tweet_vec)\n",
    "        return (tweet_vec, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the goal is to train with mini-batches, one needs to pad the sequences in each batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (data, label)\n",
    "    # sort the batch (based on tweet word count) in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # Also need to store the length of each sequence.This is later needed in order to unpad \n",
    "    # the sequences\n",
    "    seq_len = torch.Tensor([len(x) for x in sequences])\n",
    "    labels = torch.Tensor([x[1] for x in sorted_batch])\n",
    "    return sequences_padded, seq_len, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7238a",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "outputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other\n",
    "\n",
    "hidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# https://galhever.medium.com/sentiment-analysis-with-pytorch-part-4-lstm-bilstm-model-84447f6c4525\n",
    "class DisasterModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_layers, is_bidirect, emb_dim, hidden_dim, out_dim, \n",
    "                pt_emb_weights, emb_wt_update=False, drop_prob=0.5, **kwargs):\n",
    "        super(DisasterModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.output_dim = out_dim        \n",
    "        self.is_bidirect = is_bidirect\n",
    "        # Embedding layer\n",
    "        self.emb_layer = nn.Embedding(self.vocab_size, emb_dim)\n",
    "        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n",
    "        self.emb_layer.weight.data.copy_(pt_emb_weights)    \n",
    "        # whether to update the pretrained embedding layer weights during model training\n",
    "        self.emb_layer.weight.requires_grad = emb_wt_update            \n",
    "        # LSTM Layer        \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                        input_size=emb_dim, hidden_size=hidden_dim, batch_first=True, \n",
    "                        bidirectional=is_bidirect, num_layers=num_layers, dropout=drop_prob\n",
    "                        )\n",
    "        self.dropout = nn.Dropout(p = drop_prob)                        \n",
    "        \n",
    "        # If the RNN is bidirectional `num_directions` should be 2, else it should be 1.        \n",
    "        if not is_bidirect:\n",
    "            self.num_directions = 1\n",
    "            # The linear layer is for making predictions \n",
    "            # input to linear output layer is of shape num_steps, batch_size, num_hiddens\n",
    "            # and output is of shape num_steps, batch_size, output_dim\n",
    "            # Wya is of shape (output_dim, num_hiddens), a_out is of shape (num_hiddens, 1)\n",
    "            # For the last time step and one sample we have:\n",
    "            # yt_pred = np.dot(Wya, a_out) + b is of shape (output_dim, 1)\n",
    "            # replace 1 with m (batch_size) and add num_steps as the first dimension to have\n",
    "            # vectorized form of the output (num_steps, batch_size, output_dim)\n",
    "            self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        else:       \n",
    "            self.num_directions = 2     \n",
    "            # We take the hidden state only from the last lstm layer, the output from lstm is [batch_size, hidden_dim * num_directions]\n",
    "            # This is fed as input to FC layer which outputs logits in shape [batch_size, output_dim]\n",
    "            self.linear = nn.Linear(self.hidden_dim * self.num_directions, self.output_dim)\n",
    "            # If we are taking hidden states from each of the lstm layers then linear layer could be like this.\n",
    "            # self.linear = nn.Linear(self.hidden_dim * self.num_directions * num_layers, self.output_dim)\n",
    "        # The activation layer which converts output to 0 or 1            \n",
    "        self.act = nn.Sigmoid()            \n",
    "\n",
    "    def forward(self, inputs, input_lengths, state):        \n",
    "        # inputs is of shape batch_size, num_steps(sequence length which is the length of\n",
    "        # longest text sequence). Each row of inputs is 1d LongTensor array of length \n",
    "        # num_steps containing word index. Using the embedding layer we want to convert\n",
    "        # each word index to its corresponding word vector of dimension emb_dim\n",
    "        batch_size = inputs.size(0)\n",
    "        num_steps = inputs.size(1)        \n",
    "        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n",
    "        embeds = self.emb_layer(inputs)        \n",
    "        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n",
    "        # which elements of the sequence are padded ones and ignore them in computation.\n",
    "        # This step is done only after the embedding step\n",
    "        embeds_pack = pack_padded_sequence(embeds, input_lengths, batch_first=True)\n",
    "        # lstm_out is of shape batch_size * num_steps * hidden_size and contains the output\n",
    "        # features (h_t) from the last layer of LSTM for each t\n",
    "        # h_n is of shape [(num_layers*num_directions), batch_size, hidden_size] and contains the final hidden \n",
    "        # state for each sample in the batch i.e. hidden state at t_end for each of the LSTM layers. Note that for a\n",
    "        # biRNN (num_directions=2) the hidden state consists for both the forward and backward hidden states at each time step,\n",
    "        # that is why num_layers is multiplied by num_directions to get the final hidden state for each lstm layer\n",
    "        # The last element in h_n indexed by -1 like h_n[-1, :, :] represents the forward final hidden state of last lstm layer\n",
    "        # The last but one element in h_n indexed by -2 like h_n[-2, :, :] represents backward final hidden state of last lstm layer\n",
    "        # The last but two element in h_n indexed by -2 like h_n[-3, :, :] represents the forward final hidden state of \n",
    "        # previous to last lstm layer and so on...\n",
    "        # same for c_n as h_n except that it is the final cell state (Ct (memory cell) at t=t_end)\n",
    "        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n",
    "        # unpack the output\n",
    "        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)                \n",
    "        if self.is_bidirect:\n",
    "            # For a birection LSTM the hidden state at each time step is a concatenation of the hidden\n",
    "            # state from the forward pass (h_tfwd) and backward pass (h_tbwd). Also for a biRNN lstm_out is the \n",
    "            # hidden state from the last layer of LSTM for each t with shape (batch_size, num_steps, num_directions * hidden_size)\n",
    "            # The hidden state at final time step has shape (batch_size, hidden_size * num_directions) and can be extracted like this\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "            # If we want to extract both the individual hidden states at final time step for forward pass and backward pass\n",
    "            # h_tfinal_fwd = lstm_out[:, -1, :hidden_size]\n",
    "            # h_tfinal_fwd = lstm_out[:, -1, hidden_size:]\n",
    "            \n",
    "            # Another way to extract the last hidden state for the forward and backward lstm layers\n",
    "            # in a BiRNN is to use h_n like this\n",
    "            # h_tend_fwd = h_n[-1, :, :]\n",
    "            # h_tend_bwd = h_n[-2, :, :]\n",
    "            # lstm_out = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)\n",
    "            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "            # Now that we just consider the hidden state from the final time step of each lstm layer, the num_steps dimension\n",
    "            # goes away\n",
    "            # lstm_out.shape = [batch_size, hidden_size * num_directions]\n",
    "        else:            \n",
    "            # The output from lstm layer is the hidden state from last lstm layer with shape\n",
    "            # of batch_size * hidden_dim. This can be extracted directly from h_n as below, -1 representing the last\n",
    "            # last lstm_layer       \n",
    "            lstm_out = h_n[-1, :, :]  \n",
    "            # Another way to extract the final hidden state from the last lstm layer for unidirectional LSTM\n",
    "            # lstm_out = lstm_out[:, -1, :]    \n",
    "            # lstm_out.shape = [batch_size, hidden_dim]\n",
    "\n",
    "            # The below logic is dubious and needs a relook  \n",
    "            # or we can extract it from lstm_out and lstm_out_len. lstm_out is of shape\n",
    "            # batch_size * num_steps * hidden_dim. Now num_steps is the max sequence length\n",
    "            # in the batch, but for items in batch for which sequence length < max sequence length\n",
    "            # we need to take the element at lstm_out_len - 1 position in dimension 2 \n",
    "            # as elements after it are padded and should be ignored. Thus instead of num_steps\n",
    "            # if for each batch item we pick the element at (lstm_out_len - 1) index we get\n",
    "            # lstm_out in the shape batch_size * hidden_dim\n",
    "            # lstm_out = [lstm_out[batch_item_index, seq_length_index, :] for batch_item_index, seq_length_index in enumerate(lstm_out_len)]\n",
    "            # lstm_out = torch.cat(lstm_out, dim=0).reshape(batch_size, self.emb_size)\n",
    "        \n",
    "        # regularize lstm output by applying dropout\n",
    "        out = self.dropout(lstm_out)        \n",
    "        # The the output Y of fully connected rnn layer has the shape of \n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). This Y is then fed as input to the \n",
    "        # output fully connected linear layer which produces the prediction in the output shape of \n",
    "        # (`num_steps` * `batch_size`, `output_dim`).        \n",
    "        output = self.linear(out)        \n",
    "        # apply sigmoid activation to convert output to probability \n",
    "        output = self.act(output)\n",
    "        return output, (h_n, c_n)\n",
    "\n",
    "    def init_state(self, device, batch_size=1):\n",
    "        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n",
    "        to zero\"\"\"\n",
    "        if not isinstance(self.lstm_layer, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            # For a biRNN the we have two initial hidden states, one for the forward direction and one for the backward direction\n",
    "            # And each RNN layer has its own initial hidden state, hence num_directions * num_layers is the total number of initial \n",
    "            # hidden states for the network\n",
    "            return torch.zeros((self.num_directions * self.num_layers,\n",
    "                                batch_size, self.hidden_dim), device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "            # hidden state for each element in the batch, c0 = initial cell state\n",
    "            # for each element in the batch\n",
    "            return (torch.zeros((self.num_directions * self.num_layers,\n",
    "                                 batch_size, self.hidden_dim), device=device),\n",
    "                    torch.zeros((self.num_directions * self.num_layers,\n",
    "                                 batch_size, self.hidden_dim), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExecutionEngine:\n",
    "    def __init__(self, model, optimizer, device, batch_size, grad_clip, eval_metric=\"accuracy\"):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.grad_clip = grad_clip\n",
    "        self.eval_metric = eval_metric\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(outputs, targets):\n",
    "        return nn.BCELoss()(outputs, targets)\n",
    "\n",
    "    # metric to measure model performance\n",
    "    def model_eval_metric(self, outputs, targets):\n",
    "        # y_pred is in the range 0 to 1. Convert it to 0 or 1 by rounding\n",
    "        outputs_round = torch.round(outputs.squeeze())\n",
    "        correct = (outputs_round == targets.squeeze()).float()\n",
    "        metric = 0.0\n",
    "        if self.eval_metric.lower() == \"accuracy\":\n",
    "            metric = correct.sum() / len(correct)\n",
    "        elif self.eval_metric.lower() == \"f1_score\":\n",
    "            metric = metrics.f1_score(targets.squeeze(), outputs_round)\n",
    "        return metric\n",
    "\n",
    "    def train_epoch(self, data_loader, init_hidden):\n",
    "        self.model.train()\n",
    "        loss_epoch = []\n",
    "        metric_epoch = [] \n",
    "        for inputs, input_lengths, labels in data_loader:\n",
    "            # create a new hidden state instance for each minibatch to avoid long gradient chains\n",
    "            # involving all previous minibatches of an epoch\n",
    "            h = tuple([e.data for e in init_hidden])\n",
    "            inputs = inputs.to(self.device)\n",
    "            input_lengths = input_lengths.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            # the last samples that do not fit in a batch will be discarded\n",
    "            if inputs.shape[0] != self.batch_size:\n",
    "                continue        \n",
    "            # forward pass on one mini batch\n",
    "            output, hidden = self.model(inputs, input_lengths, h)        \n",
    "            # compute the loss\n",
    "            loss = self.loss_fn(output.squeeze(), labels.float())\n",
    "            metric = self.model_eval_metric(output, labels)        \n",
    "            metric_epoch.append(metric.item())\n",
    "            loss_epoch.append(loss.item())\n",
    "            # zero out the model param (W and b) gradients before running backprop on this batch \n",
    "            # otherwise gradients will keep on aggregating from one batch to next\n",
    "            self.optimizer.zero_grad()\n",
    "            # run backprop to calculate param gradients (dW and db)\n",
    "            loss.backward()\n",
    "            # clip the param gradients if they exceed threshold\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            # update the parameters (W and b)\n",
    "            self.optimizer.step()            \n",
    "        return np.mean(loss_epoch), np.mean(metric_epoch)  \n",
    "\n",
    "    def evaluate_epoch(self, data_loader, init_hidden):\n",
    "        self.model.eval()\n",
    "        loss_epoch = []\n",
    "        metric_epoch = [] \n",
    "        with torch.no_grad():\n",
    "            for inputs, input_lengths, labels in data_loader:\n",
    "                h = tuple([e.data for e in init_hidden])\n",
    "                inputs = inputs.to(self.device)\n",
    "                input_lengths = input_lengths.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                if inputs.shape[0] != self.batch_size:\n",
    "                    continue                    \n",
    "                output, hidden = self.model(inputs, input_lengths, h)        \n",
    "                loss = self.loss_fn(output.squeeze(), labels.float())\n",
    "                metric = self.model_eval_metric(output, labels)\n",
    "                metric_epoch.append(metric.item())\n",
    "                loss_epoch.append(loss.item())            \n",
    "        return np.mean(loss_epoch), np.mean(metric_epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exec_time(exec_time_secs):\n",
    "    if exec_time_secs < 60:\n",
    "        return f\"{round(exec_time_secs, 2)} seconds\"\n",
    "    exec_time_rem_sec = exec_time_secs % 60\n",
    "    exec_time_min = int((exec_time_secs - exec_time_rem_sec) / 60)\n",
    "    return f\"{exec_time_min} min {round(exec_time_rem_sec, 2)} seconds\"\n",
    "\n",
    "def print_epoch_stats(epoch, epoch_run_time, train_loss, train_metric, val_loss, val_metric):\n",
    "    print(f\"=======================================================\")\n",
    "    print(f\"Epoch {epoch} execution time = {epoch_run_time}:\")    \n",
    "    print(f\"Training loss = {round(train_loss, 4)}, training accuracy = {round(train_metric, 4)}\")    \n",
    "    print(f\"Validation loss = {round(val_loss, 4)}, validation accuracy = {round(val_metric, 4)}\")        \n",
    "    print(f\"=======================================================\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "VOCAB_SIZE = len(tweet_vocab)\n",
    "EMB_DIM = 200    \n",
    "OUT_DIM = 1    \n",
    "BATCH_SIZE = 128\n",
    "GRAD_CLIP = 5\n",
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 20 \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "MODEL_EVAL_METRIC = \"accuracy\"\n",
    "# model hyperparameters\n",
    "model_params = {\n",
    "    \"hidden_dim\": 141, \n",
    "    \"num_layers\": 2, \n",
    "    \"is_bidirectional\": True, \n",
    "    \"drop_out\": 0.4258,\n",
    "    \"learning_rate\": 0.000366\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_train = train_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_train = train_df[\"target\"].to_numpy()\n",
    "    X_valid = valid_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_valid = valid_df[\"target\"].to_numpy()\n",
    "    ds_train = VectorizedTweetDataSet(X_train, y_train)\n",
    "    ds_valid = VectorizedTweetDataSet(X_valid, y_valid)\n",
    "    dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, df, params, save_model=False):\n",
    "    train_dl, val_dl = get_fold_dls(fold, df)\n",
    "    val_loss_min = np.Inf   \n",
    "    model = DisasterModel(\n",
    "                vocab_size = VOCAB_SIZE, \n",
    "                emb_dim = EMB_DIM, \n",
    "                out_dim = OUT_DIM, \n",
    "                pt_emb_weights = pt_emb_weights,\n",
    "                num_layers = params[\"num_layers\"], \n",
    "                is_bidirect = params[\"is_bidirectional\"],  \n",
    "                hidden_dim = params[\"hidden_dim\"], \n",
    "                drop_prob = params[\"drop_out\"]).to(DEVICE)\n",
    "    #print(model)\n",
    "    print(f\"Running trainig for fold {fold}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    init_hidden = model.init_state(device = DEVICE, batch_size = BATCH_SIZE)        \n",
    "    mee = ModelExecutionEngine(\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            device=DEVICE, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            grad_clip=GRAD_CLIP,            \n",
    "            eval_metric = MODEL_EVAL_METRIC\n",
    "            )\n",
    "    # number of epoch iterations with the validation loss not decreasing \n",
    "    # before training process is terminated without completing the total number of epochs             \n",
    "    early_stopping_iter = 10            \n",
    "    early_stopping_counter = 0     \n",
    "    model_exec_stats = {\n",
    "        \"all_train_loss\": [],\n",
    "        \"all_train_metric\": [],\n",
    "        \"all_val_loss\": [],\n",
    "        \"all_val_metric\": []\n",
    "        }       \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_start_time = time.time()\n",
    "        train_loss, train_metric = mee.train_epoch(train_dl, init_hidden)\n",
    "        model_exec_stats[\"all_train_loss\"].append(train_loss)\n",
    "        model_exec_stats[\"all_train_metric\"].append(train_metric)        \n",
    "        val_loss, val_metric = mee.evaluate_epoch(val_dl, init_hidden)\n",
    "        model_exec_stats[\"all_val_loss\"].append(val_loss)\n",
    "        model_exec_stats[\"all_val_metric\"].append(val_metric)\n",
    "        val_end_time = time.time()\n",
    "        epoch_run_time = get_exec_time(val_end_time - train_start_time)\n",
    "        print_epoch_stats(epoch, epoch_run_time, train_loss, train_metric, val_loss, val_metric)  \n",
    "        if val_loss < val_loss_min:                        \n",
    "            if save_model:\n",
    "                print(f\"Validation loss decreased from \" +\n",
    "                f\"{round(val_loss_min, 6)} --> {round(val_loss, 6)}. Saving model...\")                \n",
    "                torch.save(model, \"best_model.pt\")                \n",
    "            val_loss_min = val_loss\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            break\n",
    "    return val_loss_min, model_exec_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tweet_ds = VectorizedTweetDataSet(df_train[\"vectorized_tweet\"].values, df_train[\"target\"].values)\n",
    "# # split the tweet_ds into train and validation datasets with 80:20 ratio\n",
    "# train_idx, val_idx = train_test_split(list(range(len(tweet_ds))), test_size=0.2, random_state=42)\n",
    "# train_ds = Subset(tweet_ds, train_idx)\n",
    "# val_ds = Subset(tweet_ds, val_idx)\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "# val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_val_metrics(ax, train_metric, val_metric, metric_name):\n",
    "    ax.plot(train_metric, label = f\"training {metric_name}\")\n",
    "    ax.plot(val_metric, label = f\"validation {metric_name}\")\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(f\"{metric_name} vs epochs\")\n",
    "    ax.legend()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss, train_acc, val_loss, val_acc = training_loop(\n",
    "#                                             train_dl, val_dl, model, loss_fn, optimizer, \n",
    "#                                             num_epochs=35, batch_size=batch_size\n",
    "#                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trainig for fold 0\n",
      "=======================================================\n",
      "Epoch 0 execution time = 42.55 seconds:\n",
      "Training loss = 0.6223, training accuracy = 0.6469\n",
      "Validation loss = 0.5302, validation accuracy = 0.7415\n",
      "=======================================================\n",
      "Validation loss decreased from inf --> 0.530228. Saving model...\n",
      "=======================================================\n",
      "Epoch 1 execution time = 42.28 seconds:\n",
      "Training loss = 0.4676, training accuracy = 0.7944\n",
      "Validation loss = 0.4515, validation accuracy = 0.7947\n",
      "=======================================================\n",
      "Validation loss decreased from 0.530228 --> 0.451535. Saving model...\n",
      "=======================================================\n",
      "Epoch 2 execution time = 42.91 seconds:\n",
      "Training loss = 0.4324, training accuracy = 0.8088\n",
      "Validation loss = 0.4381, validation accuracy = 0.8089\n",
      "=======================================================\n",
      "Validation loss decreased from 0.451535 --> 0.438099. Saving model...\n",
      "=======================================================\n",
      "Epoch 3 execution time = 44.21 seconds:\n",
      "Training loss = 0.4196, training accuracy = 0.8165\n",
      "Validation loss = 0.4245, validation accuracy = 0.8175\n",
      "=======================================================\n",
      "Validation loss decreased from 0.438099 --> 0.424528. Saving model...\n",
      "=======================================================\n",
      "Epoch 4 execution time = 35.87 seconds:\n",
      "Training loss = 0.4044, training accuracy = 0.8243\n",
      "Validation loss = 0.4204, validation accuracy = 0.8118\n",
      "=======================================================\n",
      "Validation loss decreased from 0.424528 --> 0.420408. Saving model...\n",
      "=======================================================\n",
      "Epoch 5 execution time = 37.08 seconds:\n",
      "Training loss = 0.3893, training accuracy = 0.8251\n",
      "Validation loss = 0.4267, validation accuracy = 0.8139\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 execution time = 41.34 seconds:\n",
      "Training loss = 0.3819, training accuracy = 0.8326\n",
      "Validation loss = 0.4299, validation accuracy = 0.8153\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 execution time = 34.81 seconds:\n",
      "Training loss = 0.3687, training accuracy = 0.8409\n",
      "Validation loss = 0.427, validation accuracy = 0.8082\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 execution time = 37.48 seconds:\n",
      "Training loss = 0.3598, training accuracy = 0.8418\n",
      "Validation loss = 0.4387, validation accuracy = 0.8196\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 execution time = 37.72 seconds:\n",
      "Training loss = 0.3559, training accuracy = 0.8474\n",
      "Validation loss = 0.4537, validation accuracy = 0.8061\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 execution time = 37.4 seconds:\n",
      "Training loss = 0.3445, training accuracy = 0.8526\n",
      "Validation loss = 0.4706, validation accuracy = 0.7905\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 execution time = 36.46 seconds:\n",
      "Training loss = 0.3224, training accuracy = 0.864\n",
      "Validation loss = 0.461, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 execution time = 38.45 seconds:\n",
      "Training loss = 0.3114, training accuracy = 0.8664\n",
      "Validation loss = 0.4582, validation accuracy = 0.7997\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 execution time = 34.49 seconds:\n",
      "Training loss = 0.3018, training accuracy = 0.8762\n",
      "Validation loss = 0.4971, validation accuracy = 0.7947\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 14 execution time = 37.06 seconds:\n",
      "Training loss = 0.2823, training accuracy = 0.8836\n",
      "Validation loss = 0.521, validation accuracy = 0.7969\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 15 execution time = 35.12 seconds:\n",
      "Training loss = 0.2714, training accuracy = 0.8891\n",
      "Validation loss = 0.5154, validation accuracy = 0.7812\n",
      "=======================================================\n",
      "Running trainig for fold 1\n",
      "=======================================================\n",
      "Epoch 0 execution time = 36.84 seconds:\n",
      "Training loss = 0.62, training accuracy = 0.6476\n",
      "Validation loss = 0.5028, validation accuracy = 0.7599\n",
      "=======================================================\n",
      "Validation loss decreased from inf --> 0.502832. Saving model...\n",
      "=======================================================\n",
      "Epoch 1 execution time = 37.8 seconds:\n",
      "Training loss = 0.4708, training accuracy = 0.7916\n",
      "Validation loss = 0.4406, validation accuracy = 0.8075\n",
      "=======================================================\n",
      "Validation loss decreased from 0.502832 --> 0.44061. Saving model...\n",
      "=======================================================\n",
      "Epoch 2 execution time = 38.59 seconds:\n",
      "Training loss = 0.4341, training accuracy = 0.8088\n",
      "Validation loss = 0.4321, validation accuracy = 0.8125\n",
      "=======================================================\n",
      "Validation loss decreased from 0.44061 --> 0.432122. Saving model...\n",
      "=======================================================\n",
      "Epoch 3 execution time = 36.48 seconds:\n",
      "Training loss = 0.413, training accuracy = 0.8183\n",
      "Validation loss = 0.4234, validation accuracy = 0.8182\n",
      "=======================================================\n",
      "Validation loss decreased from 0.432122 --> 0.423362. Saving model...\n",
      "=======================================================\n",
      "Epoch 4 execution time = 37.87 seconds:\n",
      "Training loss = 0.3992, training accuracy = 0.8246\n",
      "Validation loss = 0.4295, validation accuracy = 0.8189\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 5 execution time = 35.49 seconds:\n",
      "Training loss = 0.3896, training accuracy = 0.8296\n",
      "Validation loss = 0.4292, validation accuracy = 0.8118\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 execution time = 36.85 seconds:\n",
      "Training loss = 0.381, training accuracy = 0.8303\n",
      "Validation loss = 0.4312, validation accuracy = 0.8182\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 execution time = 36.7 seconds:\n",
      "Training loss = 0.375, training accuracy = 0.8363\n",
      "Validation loss = 0.4263, validation accuracy = 0.8104\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 execution time = 35.93 seconds:\n",
      "Training loss = 0.3593, training accuracy = 0.8421\n",
      "Validation loss = 0.449, validation accuracy = 0.8075\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 execution time = 35.61 seconds:\n",
      "Training loss = 0.3469, training accuracy = 0.8467\n",
      "Validation loss = 0.4621, validation accuracy = 0.8118\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 execution time = 35.59 seconds:\n",
      "Training loss = 0.339, training accuracy = 0.8484\n",
      "Validation loss = 0.4351, validation accuracy = 0.8203\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 execution time = 39.45 seconds:\n",
      "Training loss = 0.3288, training accuracy = 0.8595\n",
      "Validation loss = 0.4858, validation accuracy = 0.8161\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 execution time = 42.72 seconds:\n",
      "Training loss = 0.3088, training accuracy = 0.8717\n",
      "Validation loss = 0.4943, validation accuracy = 0.8139\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 execution time = 43.14 seconds:\n",
      "Training loss = 0.2957, training accuracy = 0.8758\n",
      "Validation loss = 0.4928, validation accuracy = 0.804\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 14 execution time = 43.02 seconds:\n",
      "Training loss = 0.2834, training accuracy = 0.8798\n",
      "Validation loss = 0.5657, validation accuracy = 0.7947\n",
      "=======================================================\n",
      "Running trainig for fold 2\n",
      "=======================================================\n",
      "Epoch 0 execution time = 42.81 seconds:\n",
      "Training loss = 0.6176, training accuracy = 0.6594\n",
      "Validation loss = 0.4975, validation accuracy = 0.7678\n",
      "=======================================================\n",
      "Validation loss decreased from inf --> 0.497505. Saving model...\n",
      "=======================================================\n",
      "Epoch 1 execution time = 43.2 seconds:\n",
      "Training loss = 0.4689, training accuracy = 0.7904\n",
      "Validation loss = 0.4473, validation accuracy = 0.7997\n",
      "=======================================================\n",
      "Validation loss decreased from 0.497505 --> 0.447314. Saving model...\n",
      "=======================================================\n",
      "Epoch 2 execution time = 42.59 seconds:\n",
      "Training loss = 0.4353, training accuracy = 0.8133\n",
      "Validation loss = 0.4493, validation accuracy = 0.7969\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 3 execution time = 39.73 seconds:\n",
      "Training loss = 0.417, training accuracy = 0.8201\n",
      "Validation loss = 0.4387, validation accuracy = 0.8004\n",
      "=======================================================\n",
      "Validation loss decreased from 0.447314 --> 0.438695. Saving model...\n",
      "=======================================================\n",
      "Epoch 4 execution time = 33.97 seconds:\n",
      "Training loss = 0.4028, training accuracy = 0.828\n",
      "Validation loss = 0.429, validation accuracy = 0.8047\n",
      "=======================================================\n",
      "Validation loss decreased from 0.438695 --> 0.429007. Saving model...\n",
      "=======================================================\n",
      "Epoch 5 execution time = 34.25 seconds:\n",
      "Training loss = 0.3914, training accuracy = 0.8346\n",
      "Validation loss = 0.4336, validation accuracy = 0.7997\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 execution time = 38.35 seconds:\n",
      "Training loss = 0.3785, training accuracy = 0.8389\n",
      "Validation loss = 0.4454, validation accuracy = 0.8011\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 execution time = 37.39 seconds:\n",
      "Training loss = 0.3739, training accuracy = 0.8436\n",
      "Validation loss = 0.4582, validation accuracy = 0.8082\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 execution time = 38.19 seconds:\n",
      "Training loss = 0.3602, training accuracy = 0.8489\n",
      "Validation loss = 0.4281, validation accuracy = 0.8168\n",
      "=======================================================\n",
      "Validation loss decreased from 0.429007 --> 0.42807. Saving model...\n",
      "=======================================================\n",
      "Epoch 9 execution time = 37.24 seconds:\n",
      "Training loss = 0.3473, training accuracy = 0.8567\n",
      "Validation loss = 0.4563, validation accuracy = 0.804\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 execution time = 35.46 seconds:\n",
      "Training loss = 0.3357, training accuracy = 0.8637\n",
      "Validation loss = 0.4537, validation accuracy = 0.8132\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 execution time = 39.28 seconds:\n",
      "Training loss = 0.3191, training accuracy = 0.8705\n",
      "Validation loss = 0.4776, validation accuracy = 0.8011\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 execution time = 35.98 seconds:\n",
      "Training loss = 0.3105, training accuracy = 0.8732\n",
      "Validation loss = 0.4773, validation accuracy = 0.8054\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 execution time = 37.75 seconds:\n",
      "Training loss = 0.295, training accuracy = 0.8831\n",
      "Validation loss = 0.475, validation accuracy = 0.8026\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 14 execution time = 36.73 seconds:\n",
      "Training loss = 0.2847, training accuracy = 0.8895\n",
      "Validation loss = 0.4848, validation accuracy = 0.8047\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 15 execution time = 36.46 seconds:\n",
      "Training loss = 0.2622, training accuracy = 0.8986\n",
      "Validation loss = 0.5166, validation accuracy = 0.7955\n",
      "=======================================================\n",
      "Running trainig for fold 3\n",
      "=======================================================\n",
      "Epoch 0 execution time = 36.3 seconds:\n",
      "Training loss = 0.6168, training accuracy = 0.6476\n",
      "Validation loss = 0.5267, validation accuracy = 0.767\n",
      "=======================================================\n",
      "Validation loss decreased from inf --> 0.5267. Saving model...\n",
      "=======================================================\n",
      "Epoch 1 execution time = 35.32 seconds:\n",
      "Training loss = 0.4733, training accuracy = 0.7896\n",
      "Validation loss = 0.4458, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "Validation loss decreased from 0.5267 --> 0.445809. Saving model...\n",
      "=======================================================\n",
      "Epoch 2 execution time = 35.43 seconds:\n",
      "Training loss = 0.4296, training accuracy = 0.8122\n",
      "Validation loss = 0.43, validation accuracy = 0.8104\n",
      "=======================================================\n",
      "Validation loss decreased from 0.445809 --> 0.429995. Saving model...\n",
      "=======================================================\n",
      "Epoch 3 execution time = 36.06 seconds:\n",
      "Training loss = 0.4101, training accuracy = 0.8205\n",
      "Validation loss = 0.4225, validation accuracy = 0.8168\n",
      "=======================================================\n",
      "Validation loss decreased from 0.429995 --> 0.422502. Saving model...\n",
      "=======================================================\n",
      "Epoch 4 execution time = 37.65 seconds:\n",
      "Training loss = 0.3978, training accuracy = 0.8283\n",
      "Validation loss = 0.4462, validation accuracy = 0.8047\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 5 execution time = 37.34 seconds:\n",
      "Training loss = 0.3922, training accuracy = 0.8318\n",
      "Validation loss = 0.4226, validation accuracy = 0.8104\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 execution time = 37.96 seconds:\n",
      "Training loss = 0.3801, training accuracy = 0.8359\n",
      "Validation loss = 0.4435, validation accuracy = 0.7997\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 execution time = 37.87 seconds:\n",
      "Training loss = 0.3692, training accuracy = 0.8404\n",
      "Validation loss = 0.4447, validation accuracy = 0.7969\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 execution time = 38.65 seconds:\n",
      "Training loss = 0.3551, training accuracy = 0.8507\n",
      "Validation loss = 0.4455, validation accuracy = 0.799\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 execution time = 42.95 seconds:\n",
      "Training loss = 0.3463, training accuracy = 0.8544\n",
      "Validation loss = 0.4618, validation accuracy = 0.7926\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 execution time = 37.17 seconds:\n",
      "Training loss = 0.3373, training accuracy = 0.8624\n",
      "Validation loss = 0.4793, validation accuracy = 0.7997\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 execution time = 39.21 seconds:\n",
      "Training loss = 0.3215, training accuracy = 0.8677\n",
      "Validation loss = 0.4842, validation accuracy = 0.7962\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 execution time = 38.1 seconds:\n",
      "Training loss = 0.3063, training accuracy = 0.8787\n",
      "Validation loss = 0.4825, validation accuracy = 0.8047\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 execution time = 36.42 seconds:\n",
      "Training loss = 0.2939, training accuracy = 0.8838\n",
      "Validation loss = 0.5255, validation accuracy = 0.8011\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 14 execution time = 36.5 seconds:\n",
      "Training loss = 0.2865, training accuracy = 0.8855\n",
      "Validation loss = 0.5135, validation accuracy = 0.7962\n",
      "=======================================================\n",
      "Running trainig for fold 4\n",
      "=======================================================\n",
      "Epoch 0 execution time = 33.29 seconds:\n",
      "Training loss = 0.616, training accuracy = 0.6586\n",
      "Validation loss = 0.5061, validation accuracy = 0.7663\n",
      "=======================================================\n",
      "Validation loss decreased from inf --> 0.506146. Saving model...\n",
      "=======================================================\n",
      "Epoch 1 execution time = 34.92 seconds:\n",
      "Training loss = 0.4669, training accuracy = 0.7926\n",
      "Validation loss = 0.4561, validation accuracy = 0.8011\n",
      "=======================================================\n",
      "Validation loss decreased from 0.506146 --> 0.456139. Saving model...\n",
      "=======================================================\n",
      "Epoch 2 execution time = 33.77 seconds:\n",
      "Training loss = 0.4264, training accuracy = 0.8102\n",
      "Validation loss = 0.4418, validation accuracy = 0.8089\n",
      "=======================================================\n",
      "Validation loss decreased from 0.456139 --> 0.441759. Saving model...\n",
      "=======================================================\n",
      "Epoch 3 execution time = 34.18 seconds:\n",
      "Training loss = 0.4097, training accuracy = 0.8221\n",
      "Validation loss = 0.4556, validation accuracy = 0.8033\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 4 execution time = 38.69 seconds:\n",
      "Training loss = 0.3983, training accuracy = 0.8266\n",
      "Validation loss = 0.4536, validation accuracy = 0.8082\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 5 execution time = 39.29 seconds:\n",
      "Training loss = 0.3891, training accuracy = 0.8363\n",
      "Validation loss = 0.4544, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 execution time = 39.89 seconds:\n",
      "Training loss = 0.3742, training accuracy = 0.8403\n",
      "Validation loss = 0.457, validation accuracy = 0.7969\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 execution time = 34.2 seconds:\n",
      "Training loss = 0.3624, training accuracy = 0.8471\n",
      "Validation loss = 0.4609, validation accuracy = 0.8104\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 execution time = 35.9 seconds:\n",
      "Training loss = 0.3597, training accuracy = 0.8459\n",
      "Validation loss = 0.4643, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 execution time = 37.43 seconds:\n",
      "Training loss = 0.3424, training accuracy = 0.8559\n",
      "Validation loss = 0.4881, validation accuracy = 0.7947\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 execution time = 35.62 seconds:\n",
      "Training loss = 0.3287, training accuracy = 0.8619\n",
      "Validation loss = 0.4753, validation accuracy = 0.7976\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 execution time = 36.0 seconds:\n",
      "Training loss = 0.3164, training accuracy = 0.8672\n",
      "Validation loss = 0.4963, validation accuracy = 0.7976\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 execution time = 35.19 seconds:\n",
      "Training loss = 0.3147, training accuracy = 0.8692\n",
      "Validation loss = 0.4923, validation accuracy = 0.8011\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 execution time = 35.56 seconds:\n",
      "Training loss = 0.2954, training accuracy = 0.8757\n",
      "Validation loss = 0.522, validation accuracy = 0.7962\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "val_loss_min_folds = []\n",
    "metric_min_val_loss_folds = []\n",
    "for fold in range(NUM_FOLDS):\n",
    "    val_loss_min, model_exec_stats = run_training(fold, df_train, params=model_params, save_model=True)\n",
    "    min_loss_index = np.argmin(model_exec_stats[\"all_val_loss\"])\n",
    "    # metric corresponding to the minimum validation loss epoch \n",
    "    metric_min_val_loss = model_exec_stats[\"all_val_metric\"][min_loss_index]\n",
    "    val_loss_min_folds.append(val_loss_min)\n",
    "    metric_min_val_loss_folds.append(metric_min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# plot_train_val_metrics(ax1, model_exec_stats[\"all_train_loss\"], model_exec_stats[\"all_val_loss\"], \"Loss\")\n",
    "# plot_train_val_metrics(ax2, model_exec_stats[\"all_train_metric\"], model_exec_stats[\"all_val_metric\"], \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss across cross validation folds:\n",
      "[0.42040781541304156, 0.4233617051081224, 0.42806999520822003, 0.422501956874674, 0.4417589388110421]\n",
      "accuracy across cv folds:\n",
      "[0.8117897727272727, 0.8181818181818182, 0.8167613636363636, 0.8167613636363636, 0.8089488636363636]\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum validation loss across cross validation folds:\")\n",
    "print(val_loss_min_folds)\n",
    "print(f\"{MODEL_EVAL_METRIC} across cv folds:\")\n",
    "print(metric_min_val_loss_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DisasterModel(\n",
      "  (emb_layer): Embedding(17120, 200)\n",
      "  (lstm_layer): LSTM(200, 141, num_layers=2, batch_first=True, dropout=0.4258, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.4258, inplace=False)\n",
      "  (linear): Linear(in_features=564, out_features=1, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "3263\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(\"best_model.pt\")\n",
    "print(best_model)\n",
    "tweet_vocab.set_default_index(0)\n",
    "df_test[\"vectorized_tweet\"] = df_test[\"processed_text\"].apply(\n",
    "    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "    )\n",
    "\n",
    "# Do prediction with best performing model on the test set\n",
    "def predict(df_test):\n",
    "    test_output = []\n",
    "    for index, row in df_test.iterrows():    \n",
    "        vec_tweet = row[\"vectorized_tweet\"]\n",
    "        if len(vec_tweet) == 0:\n",
    "            test_output.append(0)\n",
    "            continue\n",
    "        vec_tweet_len = torch.IntTensor([len(vec_tweet)])\n",
    "        vec_tweet = vec_tweet.view(1, -1)    \n",
    "        #print(vec_tweet, vec_tweet_len)\n",
    "        output, (h_n,c_n) = best_model(vec_tweet, vec_tweet_len, state=None)\n",
    "        #print(output)\n",
    "        test_output.append(round(output.item()))    \n",
    "    return test_output        \n",
    "\n",
    "test_output = predict(df_test)\n",
    "print(len(test_output))\n",
    "\n",
    "df_submission = pd.read_csv('./data/submission.csv')\n",
    "df_submission['target']= test_output\n",
    "df_submission.to_csv('my_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper method to run training for hyperparameter optimization as in this case the function \n",
    "# to be optimized needs to return one float value\n",
    "def hyperparam_tune_run(train_dl, val_dl, params):\n",
    "    min_val_loss, _ = run_training(train_dl, val_dl, params)\n",
    "    return min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 17:42:51,187]\u001b[0m A new study created in memory with name: DisasterModelTuning\u001b[0m\n",
      "D:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5405437552245265 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Epoch 0 :\n",
      "Execution time on training set = 11.18 seconds \n",
      "Training loss = 0.6908, training accuracy = 0.5331\n",
      "Execution time on validation set = 0.62 seconds \n",
      "Validation loss = 0.6884, validation accuracy = 0.6122\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 1 :\n",
      "Execution time on training set = 10.21 seconds \n",
      "Training loss = 0.6875, training accuracy = 0.5687\n",
      "Execution time on validation set = 0.58 seconds \n",
      "Validation loss = 0.6852, validation accuracy = 0.6243\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 2 :\n",
      "Execution time on training set = 10.18 seconds \n",
      "Training loss = 0.6833, training accuracy = 0.5961\n",
      "Execution time on validation set = 0.61 seconds \n",
      "Validation loss = 0.682, validation accuracy = 0.6186\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 3 :\n",
      "Execution time on training set = 10.4 seconds \n",
      "Training loss = 0.6816, training accuracy = 0.6012\n",
      "Execution time on validation set = 0.56 seconds \n",
      "Validation loss = 0.6798, validation accuracy = 0.6158\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 4 :\n",
      "Execution time on training set = 10.02 seconds \n",
      "Training loss = 0.6784, training accuracy = 0.6109\n",
      "Execution time on validation set = 0.58 seconds \n",
      "Validation loss = 0.676, validation accuracy = 0.6229\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 5 :\n",
      "Execution time on training set = 10.25 seconds \n",
      "Training loss = 0.6761, training accuracy = 0.6207\n",
      "Execution time on validation set = 0.53 seconds \n",
      "Validation loss = 0.6736, validation accuracy = 0.6179\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 :\n",
      "Execution time on training set = 10.06 seconds \n",
      "Training loss = 0.6734, training accuracy = 0.6192\n",
      "Execution time on validation set = 0.62 seconds \n",
      "Validation loss = 0.6717, validation accuracy = 0.6122\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 :\n",
      "Execution time on training set = 10.13 seconds \n",
      "Training loss = 0.6702, training accuracy = 0.6288\n",
      "Execution time on validation set = 0.59 seconds \n",
      "Validation loss = 0.668, validation accuracy = 0.6172\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 :\n",
      "Execution time on training set = 10.14 seconds \n",
      "Training loss = 0.666, training accuracy = 0.6336\n",
      "Execution time on validation set = 0.55 seconds \n",
      "Validation loss = 0.6665, validation accuracy = 0.6108\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 :\n",
      "Execution time on training set = 10.19 seconds \n",
      "Training loss = 0.664, training accuracy = 0.6316\n",
      "Execution time on validation set = 0.57 seconds \n",
      "Validation loss = 0.6622, validation accuracy = 0.6236\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 :\n",
      "Execution time on training set = 10.06 seconds \n",
      "Training loss = 0.6614, training accuracy = 0.635\n",
      "Execution time on validation set = 0.54 seconds \n",
      "Validation loss = 0.6567, validation accuracy = 0.6357\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 :\n",
      "Execution time on training set = 10.01 seconds \n",
      "Training loss = 0.6572, training accuracy = 0.6433\n",
      "Execution time on validation set = 0.62 seconds \n",
      "Validation loss = 0.6541, validation accuracy = 0.6378\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 :\n",
      "Execution time on training set = 10.23 seconds \n",
      "Training loss = 0.6531, training accuracy = 0.643\n",
      "Execution time on validation set = 0.61 seconds \n",
      "Validation loss = 0.6502, validation accuracy = 0.6442\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 :\n",
      "Execution time on training set = 10.32 seconds \n",
      "Training loss = 0.6493, training accuracy = 0.6523\n",
      "Execution time on validation set = 0.74 seconds \n",
      "Validation loss = 0.6478, validation accuracy = 0.6442\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 14 :\n",
      "Execution time on training set = 10.01 seconds \n",
      "Training loss = 0.6452, training accuracy = 0.6564\n",
      "Execution time on validation set = 0.51 seconds \n",
      "Validation loss = 0.6399, validation accuracy = 0.6676\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 15 :\n",
      "Execution time on training set = 9.99 seconds \n",
      "Training loss = 0.6395, training accuracy = 0.6629\n",
      "Execution time on validation set = 0.55 seconds \n",
      "Validation loss = 0.6353, validation accuracy = 0.6776\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 16 :\n",
      "Execution time on training set = 11.81 seconds \n",
      "Training loss = 0.634, training accuracy = 0.6782\n",
      "Execution time on validation set = 0.58 seconds \n",
      "Validation loss = 0.63, validation accuracy = 0.6946\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 17 :\n",
      "Execution time on training set = 10.92 seconds \n",
      "Training loss = 0.6269, training accuracy = 0.6843\n",
      "Execution time on validation set = 0.62 seconds \n",
      "Validation loss = 0.6214, validation accuracy = 0.7116\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 18 :\n",
      "Execution time on training set = 12.16 seconds \n",
      "Training loss = 0.6177, training accuracy = 0.6988\n",
      "Execution time on validation set = 0.65 seconds \n",
      "Validation loss = 0.6128, validation accuracy = 0.7102\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-21 17:46:33,787]\u001b[0m Trial 0 finished with value: 0.6011451211842623 and parameters: {'hidden_dim': 216, 'drop_out': 0.5405437552245265, 'learning_rate': 6.970240235440715e-06, 'num_layers': 1}. Best is trial 0 with value: 0.6011451211842623.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Epoch 19 :\n",
      "Execution time on training set = 12.39 seconds \n",
      "Training loss = 0.6085, training accuracy = 0.7151\n",
      "Execution time on validation set = 0.63 seconds \n",
      "Validation loss = 0.6011, validation accuracy = 0.7379\n",
      "=======================================================\n",
      "loss at end of trial 0 execution = 0.6011451211842623\n",
      "trial 0 params = {'hidden_dim': 216, 'drop_out': 0.5405437552245265, 'learning_rate': 6.970240235440715e-06, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3538461059451413 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Epoch 0 :\n",
      "Execution time on training set = 26.76 seconds \n",
      "Training loss = 0.6721, training accuracy = 0.5926\n",
      "Execution time on validation set = 1.22 seconds \n",
      "Validation loss = 0.6448, validation accuracy = 0.6236\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 1 :\n",
      "Execution time on training set = 24.75 seconds \n",
      "Training loss = 0.592, training accuracy = 0.7051\n",
      "Execution time on validation set = 1.19 seconds \n",
      "Validation loss = 0.5336, validation accuracy = 0.7543\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 2 :\n",
      "Execution time on training set = 25.31 seconds \n",
      "Training loss = 0.4956, training accuracy = 0.7763\n",
      "Execution time on validation set = 1.3 seconds \n",
      "Validation loss = 0.467, validation accuracy = 0.8004\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 3 :\n",
      "Execution time on training set = 25.5 seconds \n",
      "Training loss = 0.4642, training accuracy = 0.7949\n",
      "Execution time on validation set = 1.09 seconds \n",
      "Validation loss = 0.4633, validation accuracy = 0.7976\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 4 :\n",
      "Execution time on training set = 25.5 seconds \n",
      "Training loss = 0.4463, training accuracy = 0.8032\n",
      "Execution time on validation set = 1.09 seconds \n",
      "Validation loss = 0.4517, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 5 :\n",
      "Execution time on training set = 25.02 seconds \n",
      "Training loss = 0.4347, training accuracy = 0.8098\n",
      "Execution time on validation set = 1.2 seconds \n",
      "Validation loss = 0.4501, validation accuracy = 0.8018\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 6 :\n",
      "Execution time on training set = 23.97 seconds \n",
      "Training loss = 0.4297, training accuracy = 0.8145\n",
      "Execution time on validation set = 1.05 seconds \n",
      "Validation loss = 0.4466, validation accuracy = 0.8082\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 7 :\n",
      "Execution time on training set = 23.66 seconds \n",
      "Training loss = 0.4208, training accuracy = 0.813\n",
      "Execution time on validation set = 1.06 seconds \n",
      "Validation loss = 0.4365, validation accuracy = 0.8118\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 8 :\n",
      "Execution time on training set = 24.47 seconds \n",
      "Training loss = 0.4194, training accuracy = 0.8157\n",
      "Execution time on validation set = 1.03 seconds \n",
      "Validation loss = 0.4335, validation accuracy = 0.8104\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 9 :\n",
      "Execution time on training set = 23.51 seconds \n",
      "Training loss = 0.4128, training accuracy = 0.8208\n",
      "Execution time on validation set = 1.16 seconds \n",
      "Validation loss = 0.4399, validation accuracy = 0.8132\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 10 :\n",
      "Execution time on training set = 24.76 seconds \n",
      "Training loss = 0.4111, training accuracy = 0.8195\n",
      "Execution time on validation set = 1.09 seconds \n",
      "Validation loss = 0.429, validation accuracy = 0.8132\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 11 :\n",
      "Execution time on training set = 26.84 seconds \n",
      "Training loss = 0.4076, training accuracy = 0.8208\n",
      "Execution time on validation set = 1.08 seconds \n",
      "Validation loss = 0.4378, validation accuracy = 0.8146\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 12 :\n",
      "Execution time on training set = 24.82 seconds \n",
      "Training loss = 0.4011, training accuracy = 0.825\n",
      "Execution time on validation set = 1.08 seconds \n",
      "Validation loss = 0.455, validation accuracy = 0.8146\n",
      "=======================================================\n",
      "=======================================================\n",
      "Epoch 13 :\n",
      "Execution time on training set = 24.04 seconds \n",
      "Training loss = 0.4, training accuracy = 0.8263\n",
      "Execution time on validation set = 1.08 seconds \n",
      "Validation loss = 0.4202, validation accuracy = 0.8217\n",
      "=======================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-d528891ee294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"DisasterModelTuning\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best trial:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-122-d528891ee294>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"num_layers\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num_layers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     }\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparam_tune_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtrial_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loss at end of trial {trial_num} execution = {loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-f0289cbe2418>\u001b[0m in \u001b[0;36mhyperparam_tune_run\u001b[1;34m(train_dl, val_dl, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# to be optimized needs to return one float value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhyperparam_tune_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmin_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmin_val_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-852b545b2039>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(train_dl, val_dl, params, save_model)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtrain_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mmodel_exec_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"all_train_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmodel_exec_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"all_train_acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-0b228229e2c3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, data_loader, init_hidden)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# run backprop to calculate param gradients (dW and db)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;31m# clip the param gradients if they exceed threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\InstalledSoftware\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import time\n",
    "\n",
    "#[I 2021-10-21 12:06:44,242] Trial 15 finished with value: 0.4088201341421708 and parameters: \n",
    "# {'hidden_dim': 141, 'drop_out': 0.4257934114073623, 'learning_rate': 0.0003660548388149779, \n",
    "# 'num_layers': 2}. Best is trial 15 with value: 0.4088201341421708.\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 32, 512),\n",
    "        \"drop_out\": trial.suggest_uniform(\"drop_out\", 0.2, 0.7),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3),\n",
    "        #\"is_bidirectional\": trial.suggest_int(\"is_bidirectional\", 0, 1),\n",
    "        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 2)\n",
    "    }\n",
    "    loss = hyperparam_tune_run(train_dl, val_dl, params)\n",
    "    trial_num = trial.number\n",
    "    print(f\"loss at end of trial {trial_num} execution = {loss}\")\n",
    "    print(f\"trial {trial_num} params = {trial.params}\")\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"DisasterModelTuning\")    \n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best trial:\")\n",
    "print(study.best_params)\n",
    "\n",
    "#Best trial:\n",
    "#{'hidden_dim': 141, 'drop_out': 0.4257934114073623, 'learning_rate': 0.0003660548388149779, 'num_layers': 2}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ff06204c0662b9359ef4233b0e8cfcc016e07736dbe455d1edaa8487878aae2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.855353,
   "end_time": "2021-09-24T11:30:04.367162",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-24T11:29:27.511809",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
