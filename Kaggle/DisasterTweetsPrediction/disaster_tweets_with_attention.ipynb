{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using attention (Seq to One)\n",
    "We use a bidirectional LSTM as encoder and an attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits, binary_cross_entropy\n",
    "from torchmetrics import Accuracy, F1\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string\n",
    "import statistics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:\n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 256\n",
    "    EMB_SIZE = 300\n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_EPOCHS = 20\n",
    "    NUM_WORKERS = 8\n",
    "    # Whether to update the pretrained embedding weights during training process\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False    \n",
    "    PATIENCE = 6    \n",
    "    IS_BIDIRECTIONAL = True\n",
    "    # model hyperparameters\n",
    "    MODEL_HPARAMS = {\n",
    "        \"hidden_size\": 141, \n",
    "        \"num_layers\": 2,         \n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 7613\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive training examples = 3271\n",
      "No. of negative training examples = 4342\n",
      "No. of unique keywords = 222\n",
      "No of train examples with keyword not null = 7552\n"
     ]
    }
   ],
   "source": [
    "df_train_pos = df_train[df_train.target == 1]\n",
    "df_train_neg = df_train[df_train.target == 0]\n",
    "print(f\"No. of positive training examples = {len(df_train_pos)}\")\n",
    "print(f\"No. of negative training examples = {len(df_train_neg)}\")\n",
    "train_keywords_unique = df_train.keyword.unique()\n",
    "print(f\"No. of unique keywords = {len(train_keywords_unique)}\")\n",
    "df_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\n",
    "print(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[\"target\"].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df        \n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"target\", num_folds=5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct):\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "    return text\n",
    "\n",
    "def process_tweet(df, text, keyword):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n",
    "    processed_text = []\n",
    "    stop = stopwords.words(\"english\")\n",
    "    for tweet, keyword in zip(df[text], df[keyword]):\n",
    "        tweets_clean = []        \n",
    "        # remove stock market tickers like $GE\n",
    "        #tweet = tweet + \" \" + keyword\n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash #, @, ... sign from the word\n",
    "        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n",
    "        tweet = clean_special_chars(tweet, punct)\n",
    "        # remove junk characters which don't have an ascii code\n",
    "        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # tokenize tweets        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            # remove stopwords and punctuation\n",
    "            #if (word.isalpha() and len(word) > 2 and word not in stop \n",
    "            #    and word not in string.punctuation):\n",
    "                #stem_word = stemmer.stem(word)  # stemming word            \n",
    "                #lem_word = lemmatizer.lemmatize(word)\n",
    "                #tweets_clean.append(lem_word) \n",
    "                tweets_clean.append(word)\n",
    "        processed_text.append(\" \".join(tweets_clean))        \n",
    "    df['processed_text'] = np.array(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5174</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>Official Website</td>\n",
       "      <td>#HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>hse releases annual workplace facilities data have a look</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3126</td>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mh370 aircraft debris found on la reunion is from missing malaysia airlines</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword          location  \\\n",
       "50  5174  fatalities  Official Website   \n",
       "51  3126      debris               NaN   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "50                  #HSE releases annual workplace facilities data. Have a look | http://t.co/h4UshEekxm http://t.co/jNHNX3oISN   \n",
       "51  #??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ...  http://t.co/zxCORQ0A3a   \n",
       "\n",
       "    target  kfold  \\\n",
       "50       0      4   \n",
       "51       1      0   \n",
       "\n",
       "                                                                 processed_text  \\\n",
       "50                    hse releases annual workplace facilities data have a look   \n",
       "51  mh370 aircraft debris found on la reunion is from missing malaysia airlines   \n",
       "\n",
       "    prcsd_tweet_len  \n",
       "50                9  \n",
       "51               12  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values\n",
    "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n",
    "df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n",
    "process_tweet(df_train, 'text', \"keyword\")\n",
    "process_tweet(df_test, 'text', \"keyword\")\n",
    "# length of the processed tweet\n",
    "df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_train.iloc[50:52, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe word embedding for tweets\n",
    "#emb = torchtext.vocab.GloVe(name=\"twitter.27B\", dim=200)\n",
    "emb = torchtext.vocab.FastText(language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tweets vocab from training data\n",
    "def yield_tokens(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield row[\"processed_text\"].split()\n",
    "    \n",
    "tweet_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])   \n",
    "Config.VOCAB_SIZE = len(tweet_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the problem specific vocab, get the embedding vectors from the pre-trained embedding\n",
    "# for each word in vocab and return a matrix of shape vocab_size, embedding_dim. This matrix\n",
    "# will be the pretrained embedding weight matrix which we will use to create the embedding layer\n",
    "def get_vocab_pt_emb_matrix(text_vocab, emb):\n",
    "    embedding_matrix = []\n",
    "    for token in text_vocab.get_itos():\n",
    "        embedding_matrix.append(emb[token])\n",
    "    return torch.stack(embedding_matrix)\n",
    "\n",
    "pt_emb_weights = get_vocab_pt_emb_matrix(tweet_vocab, emb)\n",
    "pt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the processed tweet, i.e. replace each token in the tweet with its corresponding index\n",
    "# in the tweet vocab\n",
    "df_train[\"vectorized_tweet\"] = df_train[\"processed_text\"].apply(\n",
    "    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedTweetDataSet(Dataset):\n",
    "    def __init__(self, tweet_vecs, labels):\n",
    "        self.tweet_vecs = tweet_vecs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_vec = self.tweet_vecs[idx]\n",
    "        label = self.labels[idx]\n",
    "        tweet_len = len(tweet_vec)\n",
    "        return (tweet_vec, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train and validation data for a fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_train = train_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_train = train_df[\"target\"].to_numpy()\n",
    "    X_valid = valid_df[\"vectorized_tweet\"].to_numpy()\n",
    "    y_valid = valid_df[\"target\"].to_numpy()\n",
    "    ds_train = VectorizedTweetDataSet(X_train, y_train)\n",
    "    ds_valid = VectorizedTweetDataSet(X_valid, y_valid)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the goal is to train with mini-batches, one needs to pad the sequences in each batch. \n",
    "# In other words, given a mini-batch of size N, if the length of the largest sequence is L, \n",
    "# one needs to pad every sequence with a length of smaller than L with zeros and make their \n",
    "# lengths equal to L. Moreover, it is important that the sequences in the batch are in the \n",
    "# descending order.\n",
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (data, label)\n",
    "    # sort the batch (based on tweet word count) in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # Also need to store the length of each sequence.This is later needed in order to unpad \n",
    "    # the sequences\n",
    "    seq_len = torch.Tensor([len(x) for x in sequences])\n",
    "    labels = torch.Tensor([x[1] for x in sorted_batch])\n",
    "    return sequences_padded, seq_len, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "<img src=\"temp.jpg\" style=\"width:800px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model \n",
    "Encoder (biLSTM) => Attention layer => Fully connected layer => Sigmoid \n",
    "\n",
    "**Bidirectional RNN as encoder** <br>\n",
    "outputs is of size [src len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other. \n",
    "\n",
    "hidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
    "\n",
    "The bidirectional rnn encoder returns the hidden state from each time step as well as the final hidden state (last time step). \n",
    "\n",
    "**Attention layer** <br>\n",
    "Takes as input encoder outputs ( hidden state from each time step of last rnn layer) as well encoder final hidden state (fom the last time step).\n",
    "encoder_outputs + enc_final_hidden_state => alignment_score (use one of the methods below. We use concat and dot product implementation of alignment score)\n",
    "\n",
    "<img src=\"align_score.jpg\" style=\"width:800px;height:500px;\">\n",
    "\n",
    "Softmax(alignment_score) => attention weights <br>\n",
    "attention weights * encoder outputs => context vector <br>\n",
    "Context vector has dimensions batch_size, hidden_size and is return by the attention layer <br>\n",
    "\n",
    "**FC layer** <br>\n",
    "Input => Context vector (batch_size, hidden_size) <br>\n",
    "Output => batch_size, out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, params, hparams):\n",
    "        super().__init__()                \n",
    "        self.num_layers = hparams[\"num_layers\"]\n",
    "        self.hidden_size = hparams[\"hidden_size\"]\n",
    "        self.is_bidirect = params[\"is_bidirect\"]\n",
    "        self.num_directions = 2 if self.is_bidirect else 1\n",
    "        # Embedding layer\n",
    "        self.emb_layer = nn.Embedding(params[\"vocab_size\"], params[\"emb_size\"])\n",
    "        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n",
    "        self.emb_layer.weight.data.copy_(params[\"pt_emb_weights\"])\n",
    "        # whether to update the pretrained embedding layer weights during model training\n",
    "        self.emb_layer.weight.requires_grad = params[\"emb_wt_update\"] \n",
    "        # LSTM Layer        \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                        input_size=params[\"emb_size\"], \n",
    "                        hidden_size=self.hidden_size, \n",
    "                        batch_first=True, \n",
    "                        bidirectional=self.is_bidirect, \n",
    "                        num_layers=self.num_layers, \n",
    "                        dropout=hparams[\"drop_out\"]\n",
    "                        )\n",
    "        \n",
    "    def forward(self, inputs, input_lengths, state):        \n",
    "        # inputs = [batch_size, batch_max_seq_length]        \n",
    "        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n",
    "        embeds = self.emb_layer(inputs)        \n",
    "        # final hidden state (from last time step)\n",
    "        h_final = None        \n",
    "        # embeds = [batch_size, max_seq_length, emb_dim]        \n",
    "        embeds_pack = pack_padded_sequence(embeds, input_lengths.to(\"cpu\"), batch_first=True)                \n",
    "        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        # unpack the output\n",
    "        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)        \n",
    "        # print(f\"lstm_out.shape = {lstm_out.shape}\") # [batch_size, max_seq_length, hidden_size * num_directions]        \n",
    "        if self.is_bidirect:                        \n",
    "            h_tend_fwd = h_n[-2, :, :]\n",
    "            h_tend_bwd = h_n[-1, :, :]\n",
    "            h_final = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)            \n",
    "        else:                        \n",
    "            h_final = h_n[-1, :, :]   \n",
    "\n",
    "        # print(f\"h_final.shape = {h_final.shape}\") # [batch_size, hidden_size * num_directions]\n",
    "        return lstm_out, lstm_out_len, h_final\n",
    "\n",
    "    def init_state(self, batch_size=1):\n",
    "        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n",
    "        to zero\"\"\"\n",
    "        if not isinstance(self.lstm_layer, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "            # hidden state for each element in the batch, c0 = initial cell state\n",
    "            # for each element in the batch\n",
    "            return (torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n",
    "                    torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMethod:\n",
    "    CONCAT = \"concat\"\n",
    "    DOT = \"dot\"\n",
    "    GENERAL = \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_method = AttentionMethod.CONCAT):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_method = attn_method   \n",
    "        self.attn = None\n",
    "        self.v = None\n",
    "        self.concat_linear = None\n",
    "        if self.attn_method == AttentionMethod.CONCAT:     \n",
    "            self.attn = nn.Linear((hidden_size * 2) + (hidden_size * 2), hidden_size)\n",
    "            self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        if self.attn_method == AttentionMethod.DOT:\n",
    "            self.attn = nn.Linear((hidden_size * 2) + (hidden_size * 2), hidden_size)\n",
    "\n",
    "    def attn_concat(self, encoder_outputs, enc_final_hidden_state):        \n",
    "        batch_size, seq_length, _ = encoder_outputs.shape        \n",
    "        # add seq_length dim to enc_final_hiddden_state\n",
    "        enc_final_hidden_state = enc_final_hidden_state.unsqueeze(1)\n",
    "        # enc_final_hidden_state.shape = [batch_size, 1, enc_hidden_size * 2]\n",
    "        # now repeat the final hidden state seq_length times across dim 1 so final hidden state and encoder outputs have same dimensions\n",
    "        enc_final_hidden_state = enc_final_hidden_state.repeat(1, seq_length, 1)\n",
    "        # print(f\"enc_final_hidden_state.shape = {enc_final_hidden_state.shape}\") # [batch_size, seq_length, enc_hidden_size * 2]\n",
    "        # concat the enc final hidden state and the encoder outputs (which are nothing but enc hidden states at individual time steps)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, enc_final_hidden_state), dim=2)))\n",
    "        # print(f\"energy.shape = {energy.shape}\") # [batch_size, seq_length, enc_hidden_size]\n",
    "        # get attention vector corresponding to each source time step\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return attention\n",
    "\n",
    "    def attn_dot(self, encoder_outputs, enc_final_hidden_state):        \n",
    "        attention = torch.bmm(encoder_outputs, enc_final_hidden_state.unsqueeze(2))\n",
    "        # print(f\"attention.shape = {attention.shape}\") # [batch_size, seq_length, 1]\n",
    "        return attention.squeeze(2)\n",
    "\n",
    "    def forward(self, encoder_outputs, enc_final_hidden_state):        \n",
    "        # print(f\"encoder_outputs.shape = {encoder_outputs.shape}\") #[batch_size, seq_length, enc_hidden_size * num_directions]\n",
    "        # print(f\"enc_final_hidden_state.shape = {enc_final_hidden_state.shape}\") # [batch_size, enc_hidden_size * num_directions]        \n",
    "        attention = None\n",
    "        if self.attn_method == AttentionMethod.CONCAT:\n",
    "            attention = self.attn_concat(encoder_outputs, enc_final_hidden_state)\n",
    "        elif self.attn_method == AttentionMethod.DOT:\n",
    "            attention = self.attn_dot(encoder_outputs, enc_final_hidden_state)            \n",
    "        # print(f\"(attention.shape = {attention.shape}\") # [batch_size, seq_length]\n",
    "        attn_weights = F.softmax(attention, dim=1)\n",
    "        # print(f\"(attn_weights.shape = {attn_weights.shape}\") # [batch_size, seq_length]\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # print(f\"(attn_weights.shape = {attn_weights.shape}\") # [batch_size, 1, seq_length]\n",
    "        # apply attention weights to encoder outputs to get context vector\n",
    "        context_vector = torch.bmm(attn_weights, encoder_outputs)\n",
    "        # print(f\"(context_vector.shape = {context_vector.shape}\") # [batch_size, 1, enc_hidden_size * 2]\n",
    "        # attn_hidden = torch.tanh(self.attn(torch.cat((context_vector.squeeze(1), enc_final_hidden_state), dim=1)))\n",
    "        # print(f\"attn_hidden.shape = {attn_hidden.shape}\") # [batch_size, hidden_size]\n",
    "        return context_vector.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnAttnClassifier(nn.Module):\n",
    "    def __init__(self, params, hparams):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(params, hparams)\n",
    "        self.attention_layer = AttentionLayer(hparams[\"hidden_size\"], params[\"attn_method\"])\n",
    "        self.fc = None\n",
    "        if params[\"attn_method\"] == AttentionMethod.CONCAT:\n",
    "            self.fc = nn.Linear(hparams[\"hidden_size\"] * 2, 2)\n",
    "        elif params[\"attn_method\"] == AttentionMethod.DOT:\n",
    "            self.fc = nn.Linear(hparams[\"hidden_size\"] * 2, 2)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, input_lengths, state):\n",
    "        enc_out, enc_out_len, enc_h_final = self.encoder(inputs, input_lengths, state)\n",
    "        ctx_vec = self.attention_layer(enc_out, enc_h_final)\n",
    "        out = self.fc(ctx_vec)\n",
    "        return self.act(out)\n",
    "\n",
    "    def init_state(self):\n",
    "        return self.encoder.init_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch lightning wrapper for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetLitModel(pl.LightningModule):\n",
    "    def __init__(self, params, hparams, model_eval_metric=MODEL_EVAL_METRIC.accuracy):\n",
    "        super().__init__()\n",
    "        #self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        self.model_eval_metric = model_eval_metric\n",
    "        self.network = RnnAttnClassifier(params, hparams)            \n",
    "\n",
    "    def forward(self, tweets, tweet_lengths, state):\n",
    "        return self.network(tweets, tweet_lengths, state)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))        \n",
    "        #print(f\"targets_pred.shape = {targets_pred.shape}\")\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        train_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        train_metric = None\n",
    "        train_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)            \n",
    "            train_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            train_metric_str = \"train_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            train_metric = F1(targets_pred, targets)            \n",
    "            train_metric_str = \"train_f1\"\n",
    "        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        val_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        val_metric = None\n",
    "        val_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:\n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)\n",
    "            val_metric = Accuracy(num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            val_metric_str = \"val_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            val_metric = F1(targets_pred, targets)            \n",
    "            val_metric_str = \"val_f1\"\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom lightning callback \n",
    "To record training and validation metric values at each epoch and the best metric values across all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "# Monitor multiple metric values that are calculated either in training or validation step and return the\n",
    "# best metric values for each epoch\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n",
    "        # dictionary with metric name as key and monitor mode (min, max) as the value\n",
    "        # ( the same names used to log metric values in training and validation step)\n",
    "        self.val_metrics_to_monitor = val_metrics_to_monitor\n",
    "        self.train_metrics_to_monitor = train_metrics_to_monitor\n",
    "        # dictionary with metric_name as key and list of metric value for each epoch\n",
    "        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the best metric value for all epochs\n",
    "        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the epoch number with the best metric value\n",
    "        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n",
    "        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n",
    "        self.epoch_counter = 0           \n",
    "\n",
    "    @staticmethod\n",
    "    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n",
    "        metric_str = \"\"\n",
    "        for metric, mode in metrics_to_monitor.items():\n",
    "            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n",
    "            metric_str += f\"{metric} = {metric_value}, \"\n",
    "            metrics[metric].append(metric_value)\n",
    "            if mode == \"max\":\n",
    "                best_metric[metric] = max(metrics[metric])            \n",
    "            elif mode == \"min\":            \n",
    "                best_metric[metric] = min(metrics[metric])            \n",
    "            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n",
    "        print(metric_str[:-2])\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.epoch_counter += 1        \n",
    "        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n",
    "        print(f\"For epoch {self.epoch_counter}\")\n",
    "        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "        \"vocab_size\": Config.VOCAB_SIZE,\n",
    "        \"emb_size\": Config.EMB_SIZE,\n",
    "        \"output_size\": Config.OUT_SIZE,\n",
    "        \"pt_emb_weights\": pt_emb_weights,\n",
    "        \"emb_wt_update\": Config.EMB_WT_UPDATE,\n",
    "        \"is_bidirect\": True,\n",
    "        \"attn_method\": AttentionMethod.DOT\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")    \n",
    "    disaster_tweet_model = DisasterTweetLitModel(\n",
    "        params=model_params,        \n",
    "        hparams=Config.MODEL_HPARAMS,\n",
    "        model_eval_metric=Config.MODEL_EVAL_METRIC                \n",
    "        )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = fold_str + \"_best_model_{epoch}_{val_loss:.4f}\"\n",
    "    train_metrics_to_monitor = {\n",
    "        \"train_loss\": \"min\",\n",
    "        \"train_acc\": \"max\"\n",
    "    }\n",
    "    val_metrics_to_monitor = {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        }\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus = 1,\n",
    "        deterministic = True,\n",
    "        auto_select_gpus = True,\n",
    "        progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        auto_lr_find = True,    \n",
    "        #precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        gradient_clip_val = 1.0,        \n",
    "        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n",
    "        print(disaster_tweet_model.lr)\n",
    "    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_train_metrics = {\n",
    "        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n",
    "        for metric in train_metrics_to_monitor.keys()\n",
    "    }\n",
    "    fold_val_metrics = {\n",
    "        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n",
    "        for metric in val_metrics_to_monitor.keys()\n",
    "    }            \n",
    "    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n",
    "    return fold_train_metrics, fold_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | network | RnnAttnClassifier | 6.2 M \n",
      "----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109c8a96d5b247d1aab4f4b4274e02a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6932, val_acc = 0.5215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0b1feb97854abcb25262fc61974e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3392931281f54318bb124959abca80c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.624\n",
      "Epoch 0, global step 23: val_loss reached 0.62392 (best 0.62392), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=0_val_loss=0.6239.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6239, val_acc = 0.6408\n",
      "train_loss = 0.6667, train_acc = 0.5709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7a64068ec549d0ac989413e74c9fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.129 >= min_delta = 0.0. New best score: 0.495\n",
      "Epoch 1, global step 47: val_loss reached 0.49527 (best 0.49527), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=1_val_loss=0.4953.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.4953, val_acc = 0.7768\n",
      "train_loss = 0.5504, train_acc = 0.7455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f65996916b0461cb0c369754f2710d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.062 >= min_delta = 0.0. New best score: 0.433\n",
      "Epoch 2, global step 71: val_loss reached 0.43322 (best 0.43322), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=2_val_loss=0.4332.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.4332, val_acc = 0.8142\n",
      "train_loss = 0.4379, train_acc = 0.8156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcebed3ad994c6ebeb1c4120e02193e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.409\n",
      "Epoch 3, global step 95: val_loss reached 0.40911 (best 0.40911), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=3_val_loss=0.4091.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.4091, val_acc = 0.8332\n",
      "train_loss = 0.3796, train_acc = 0.8443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be93ce85bf24f9683234acc9765660a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.409\n",
      "Epoch 4, global step 119: val_loss reached 0.40895 (best 0.40895), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold0_best_model_epoch=4_val_loss=0.4090.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.409, val_acc = 0.8391\n",
      "train_loss = 0.3335, train_acc = 0.8695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d630b2208cc45c2be64275048c8777d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.4351, val_acc = 0.8293\n",
      "train_loss = 0.2914, train_acc = 0.8887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6997b007e184e3b9d9ec39fcfdcbf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.4555, val_acc = 0.8332\n",
      "train_loss = 0.2537, train_acc = 0.9069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95d6d9900b4403099d618d656077a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.5384, val_acc = 0.7945\n",
      "train_loss = 0.2136, train_acc = 0.922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041a16d915dd499fad1aa95bb11a0ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 215: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.5658, val_acc = 0.8142\n",
      "train_loss = 0.174, train_acc = 0.9374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb36c43154034d25bb87674f6258f9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 239: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.5968, val_acc = 0.8043\n",
      "train_loss = 0.1421, train_acc = 0.9509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb2a0da217b4b6ebf338dd00492346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.409. Signaling Trainer to stop.\n",
      "Epoch 10, global step 263: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10\n",
      "val_loss = 0.6561, val_acc = 0.8011\n",
      "train_loss = 0.1155, train_acc = 0.9616\n",
      "Best train metrics values for fold0\n",
      "{'train_loss': (0.1155, 10), 'train_acc': (0.9616, 10)}\n",
      "Best val metrics values for fold0\n",
      "{'val_loss': (0.409, 5), 'val_acc': (0.8391, 5)}\n",
      "Running training for fold1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | network | RnnAttnClassifier | 6.2 M \n",
      "----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f9480969e430f9740bfce9d00afe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6919, val_acc = 0.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e370b4dbab974fd19091e77dc20a911a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b458b4a9a54ea89bb73b6cd165c28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.613\n",
      "Epoch 0, global step 23: val_loss reached 0.61283 (best 0.61283), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold1_best_model_epoch=0_val_loss=0.6128.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6128, val_acc = 0.6671\n",
      "train_loss = 0.6589, train_acc = 0.5762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d198823ef54474b819cd48a9004ff95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.133 >= min_delta = 0.0. New best score: 0.480\n",
      "Epoch 1, global step 47: val_loss reached 0.48033 (best 0.48033), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold1_best_model_epoch=1_val_loss=0.4803.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.4803, val_acc = 0.7866\n",
      "train_loss = 0.5416, train_acc = 0.765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9d9f8a141842eaa644341805a4e07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.443\n",
      "Epoch 2, global step 71: val_loss reached 0.44309 (best 0.44309), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold1_best_model_epoch=2_val_loss=0.4431.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.4431, val_acc = 0.8109\n",
      "train_loss = 0.4361, train_acc = 0.8174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca079236b074438bae53c0ce711a7293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.436\n",
      "Epoch 3, global step 95: val_loss reached 0.43591 (best 0.43591), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold1_best_model_epoch=3_val_loss=0.4359.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.4359, val_acc = 0.8142\n",
      "train_loss = 0.3711, train_acc = 0.8499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d743a156949f434d813cc1ce43ad13ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 119: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.4462, val_acc = 0.8142\n",
      "train_loss = 0.3282, train_acc = 0.8732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ca579305fa4b3f90e9b13dc95687e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.4363, val_acc = 0.828\n",
      "train_loss = 0.2968, train_acc = 0.8918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6686d63f1c641a997c925239fd441c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.471, val_acc = 0.8168\n",
      "train_loss = 0.257, train_acc = 0.9056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcec50be484d4da692e9c1d0fb1985ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.524, val_acc = 0.8083\n",
      "train_loss = 0.2154, train_acc = 0.9235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbac821182f433ba05f59e5b9d635fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 215: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.5589, val_acc = 0.8037\n",
      "train_loss = 0.1737, train_acc = 0.9404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3671d5149b784bf4959bbba2162bbf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.436. Signaling Trainer to stop.\n",
      "Epoch 9, global step 239: val_loss was not in top 1\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | network | RnnAttnClassifier | 6.2 M \n",
      "----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.6249, val_acc = 0.7892\n",
      "train_loss = 0.1465, train_acc = 0.9504\n",
      "Best train metrics values for fold1\n",
      "{'train_loss': (0.1465, 9), 'train_acc': (0.9504, 9)}\n",
      "Best val metrics values for fold1\n",
      "{'val_loss': (0.4359, 4), 'val_acc': (0.828, 6)}\n",
      "Running training for fold2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b365ddda2fed4d008e23a4e44d08fd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6894, val_acc = 0.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0816e1eaaf8b4359af7c928db35ad77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ebd733ca904b15a09bb24ea3fde956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.625\n",
      "Epoch 0, global step 23: val_loss reached 0.62535 (best 0.62535), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold2_best_model_epoch=0_val_loss=0.6253.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6253, val_acc = 0.61\n",
      "train_loss = 0.6688, train_acc = 0.5704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec67ab04b51f4127a2369f2de3b8e03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.145 >= min_delta = 0.0. New best score: 0.481\n",
      "Epoch 1, global step 47: val_loss reached 0.48071 (best 0.48071), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold2_best_model_epoch=1_val_loss=0.4807.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.4807, val_acc = 0.8011\n",
      "train_loss = 0.5615, train_acc = 0.744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7394bdc8b5e44d09a7ae396b64f63b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 0.437\n",
      "Epoch 2, global step 71: val_loss reached 0.43678 (best 0.43678), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold2_best_model_epoch=2_val_loss=0.4368.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.4368, val_acc = 0.8168\n",
      "train_loss = 0.4374, train_acc = 0.8223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64a3d242b27482688b8542799a223c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.433\n",
      "Epoch 3, global step 95: val_loss reached 0.43276 (best 0.43276), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold2_best_model_epoch=3_val_loss=0.4328.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.4328, val_acc = 0.8234\n",
      "train_loss = 0.3775, train_acc = 0.8483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e5894cffd745619324e6b7011703a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 119: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.4348, val_acc = 0.8306\n",
      "train_loss = 0.3327, train_acc = 0.8722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25468fa501bc4ce5b2fcd6e4d1b44a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.4367, val_acc = 0.8306\n",
      "train_loss = 0.3043, train_acc = 0.8821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f55dfce0ebd4d6c967dda8324109323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.4685, val_acc = 0.8267\n",
      "train_loss = 0.268, train_acc = 0.9002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928f029482404945a85ea5b7420b15cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.4851, val_acc = 0.8207\n",
      "train_loss = 0.2209, train_acc = 0.9207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10617131ed9149b58a3d5c1a4981e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 215: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.5375, val_acc = 0.8142\n",
      "train_loss = 0.1865, train_acc = 0.9333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67856e5b7eae44779f8ce29187670437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.433. Signaling Trainer to stop.\n",
      "Epoch 9, global step 239: val_loss was not in top 1\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | network | RnnAttnClassifier | 6.2 M \n",
      "----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.588, val_acc = 0.8083\n",
      "train_loss = 0.1519, train_acc = 0.9488\n",
      "Best train metrics values for fold2\n",
      "{'train_loss': (0.1519, 9), 'train_acc': (0.9488, 9)}\n",
      "Best val metrics values for fold2\n",
      "{'val_loss': (0.4328, 4), 'val_acc': (0.8306, 5)}\n",
      "Running training for fold3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6af665af1f3473fb6232846329745d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6901, val_acc = 0.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a94da3203334922a2fd62cc873e8d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c98d12c2d240df8fd6976f76d4f2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.612\n",
      "Epoch 0, global step 23: val_loss reached 0.61153 (best 0.61153), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold3_best_model_epoch=0_val_loss=0.6115.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6115, val_acc = 0.6991\n",
      "train_loss = 0.6608, train_acc = 0.5745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a35d608e2f40889d0bb71bb2d075d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.145 >= min_delta = 0.0. New best score: 0.467\n",
      "Epoch 1, global step 47: val_loss reached 0.46658 (best 0.46658), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold3_best_model_epoch=1_val_loss=0.4666.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.4666, val_acc = 0.8062\n",
      "train_loss = 0.5269, train_acc = 0.7767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bcd9885b2d47548fee0b7a16b7d20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.421\n",
      "Epoch 2, global step 71: val_loss reached 0.42055 (best 0.42055), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold3_best_model_epoch=2_val_loss=0.4206.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.4206, val_acc = 0.8233\n",
      "train_loss = 0.4179, train_acc = 0.8278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c5f755d0864280b2a9ebb3a40e11ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 95: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.4305, val_acc = 0.8219\n",
      "train_loss = 0.3612, train_acc = 0.8572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d676e2b2a6194caf83cb0d8a6b882ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 119: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.4329, val_acc = 0.8265\n",
      "train_loss = 0.3196, train_acc = 0.877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bfab1b630f4d998bbf403d676ecf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.4504, val_acc = 0.8285\n",
      "train_loss = 0.2754, train_acc = 0.8948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b316d94caef436a8778df3896f8d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.5118, val_acc = 0.8141\n",
      "train_loss = 0.2319, train_acc = 0.915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1affef0d8e02463bab1baa9660ad6216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.601, val_acc = 0.7898\n",
      "train_loss = 0.1874, train_acc = 0.9335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9164f87b794587870055c1891bed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.421. Signaling Trainer to stop.\n",
      "Epoch 8, global step 215: val_loss was not in top 1\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | network | RnnAttnClassifier | 6.2 M \n",
      "----------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.6671, val_acc = 0.8088\n",
      "train_loss = 0.1553, train_acc = 0.9458\n",
      "Best train metrics values for fold3\n",
      "{'train_loss': (0.1553, 8), 'train_acc': (0.9458, 8)}\n",
      "Best val metrics values for fold3\n",
      "{'val_loss': (0.4206, 3), 'val_acc': (0.8285, 6)}\n",
      "Running training for fold4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08b977e16664d5b8dcd7d988677a8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6924, val_acc = 0.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51e72b4d0dc49aa8f08bf3ded606016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e91a2faa724823aac5aa8d5cd7864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.613\n",
      "Epoch 0, global step 23: val_loss reached 0.61250 (best 0.61250), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold4_best_model_epoch=0_val_loss=0.6125.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6125, val_acc = 0.6728\n",
      "train_loss = 0.6646, train_acc = 0.5736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d32290a6f4effb57895fd588b1636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.127 >= min_delta = 0.0. New best score: 0.485\n",
      "Epoch 1, global step 47: val_loss reached 0.48514 (best 0.48514), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold4_best_model_epoch=1_val_loss=0.4851.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.4851, val_acc = 0.7884\n",
      "train_loss = 0.5391, train_acc = 0.7659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ad7b51159f44fabd00a0bd6b905d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 0.446\n",
      "Epoch 2, global step 71: val_loss reached 0.44578 (best 0.44578), saving model to \"/home/bk_anupam/code/ML/NLP/Kaggle/DisasterTweetsPrediction/model/fold4_best_model_epoch=2_val_loss=0.4458.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.4458, val_acc = 0.8081\n",
      "train_loss = 0.4229, train_acc = 0.8235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f440285997954ec5b9976b66eedd0d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 95: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.447, val_acc = 0.8055\n",
      "train_loss = 0.3656, train_acc = 0.8519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a4b691f2284fb1bcdd7fdc26c0dd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 119: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.4535, val_acc = 0.8127\n",
      "train_loss = 0.3207, train_acc = 0.8741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61888972f984fe39ba3b6dde681b8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 143: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.4754, val_acc = 0.8127\n",
      "train_loss = 0.2795, train_acc = 0.8921\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790cde8d0ef940f5ae5ca64ada70401e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 167: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.4836, val_acc = 0.8095\n",
      "train_loss = 0.2407, train_acc = 0.9148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611d3ed9ab93465491ddbc58ac233146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 191: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.5408, val_acc = 0.7996\n",
      "train_loss = 0.203, train_acc = 0.931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addd3dc84afd40c485c17c0becc6e1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.446. Signaling Trainer to stop.\n",
      "Epoch 8, global step 215: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.6435, val_acc = 0.7884\n",
      "train_loss = 0.1659, train_acc = 0.9453\n",
      "Best train metrics values for fold4\n",
      "{'train_loss': (0.1659, 8), 'train_acc': (0.9453, 8)}\n",
      "Best val metrics values for fold4\n",
      "{'val_loss': (0.4458, 3), 'val_acc': (0.8127, 5)}\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "all_fold_val_loss = []\n",
    "all_fold_val_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val = get_fold_dls(fold, df_train)\n",
    "    fold_train_metrics, fold_val_metrics = run_training(fold, dl_train, dl_val, find_lr=False)    \n",
    "    all_fold_val_loss.append(fold_val_metrics[\"val_loss\"][0])\n",
    "    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"][0])\n",
    "    print(f\"Best train metrics values for fold{fold}\")    \n",
    "    print(fold_train_metrics)\n",
    "    print(f\"Best val metrics values for fold{fold}\")    \n",
    "    print(fold_val_metrics)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss across folds = [0.409, 0.4359, 0.4328, 0.4206, 0.4458]\n",
      "val accuracy across folds = [0.8391, 0.828, 0.8306, 0.8285, 0.8127]\n",
      "mean val loss across folds = 0.42882, val loss stdev across fold = 0.014271369941249515\n",
      "mean val accuracy across folds = 0.82778, val accuracy stdev across fold = 0.00954028301466995\n"
     ]
    }
   ],
   "source": [
    "#all_fold_val_loss = [x[0] for x in all_fold_val_loss]\n",
    "#all_fold_val_acc = [x[0] for x in all_fold_val_acc]\n",
    "print(f\"val loss across folds = {all_fold_val_loss}\")\n",
    "print(f\"val accuracy across folds = {all_fold_val_acc}\")\n",
    "mean_loss = statistics.mean(all_fold_val_loss)\n",
    "mean_acc = statistics.mean(all_fold_val_acc)\n",
    "std_loss = statistics.stdev(all_fold_val_loss)\n",
    "std_acc = statistics.stdev(all_fold_val_acc)\n",
    "print(f\"mean val loss across folds = {mean_loss}, val loss stdev across fold = {std_loss}\")\n",
    "print(f\"mean val accuracy across folds = {mean_acc}, val accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DisasterTweetLitModel(\n",
      "  (network): RnnAttnClassifier(\n",
      "    (encoder): Encoder(\n",
      "      (emb_layer): Embedding(17120, 300)\n",
      "      (lstm_layer): LSTM(300, 141, num_layers=2, batch_first=True, dropout=0.4258, bidirectional=True)\n",
      "    )\n",
      "    (attention_layer): AttentionLayer(\n",
      "      (attn): Linear(in_features=564, out_features=141, bias=True)\n",
      "    )\n",
      "    (fc): Linear(in_features=282, out_features=2, bias=True)\n",
      "    (act): Sigmoid()\n",
      "  )\n",
      ")\n",
      "3263\n"
     ]
    }
   ],
   "source": [
    "best_model = DisasterTweetLitModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"./model/fold0_best_model_epoch=4_val_loss=0.4090.ckpt\",\n",
    "    params = model_params,\n",
    "    hparams = Config.MODEL_HPARAMS\n",
    "    )\n",
    "print(best_model)\n",
    "tweet_vocab.set_default_index(0)\n",
    "df_test[\"vectorized_tweet\"] = df_test[\"processed_text\"].apply(\n",
    "    lambda row:torch.LongTensor(tweet_vocab.lookup_indices(row.split()))\n",
    "    )\n",
    "\n",
    "# Do prediction with best performing model on the test set\n",
    "def predict(df_test):\n",
    "    test_output = []\n",
    "    for index, row in df_test.iterrows():    \n",
    "        vec_tweet = row[\"vectorized_tweet\"]\n",
    "        if len(vec_tweet) == 0:\n",
    "            test_output.append(0)\n",
    "            continue\n",
    "        vec_tweet_len = torch.IntTensor([len(vec_tweet)])\n",
    "        vec_tweet = vec_tweet.view(1, -1)    \n",
    "        #print(vec_tweet, vec_tweet_len)\n",
    "        output = best_model(vec_tweet, vec_tweet_len, state=None)\n",
    "        #print(output)\n",
    "        test_output.append(torch.argmax(output).item())    \n",
    "    return test_output        \n",
    "\n",
    "test_output = predict(df_test)\n",
    "print(len(test_output))\n",
    "\n",
    "df_submission = pd.read_csv('./data/submission.csv')\n",
    "df_submission['target']= test_output\n",
    "df_submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
