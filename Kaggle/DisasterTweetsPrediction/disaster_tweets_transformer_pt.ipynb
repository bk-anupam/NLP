{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune BERT for text classification using pytorch and huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import DataCollatorWithPadding, AdamW, get_scheduler, set_seed, get_linear_schedule_with_warmup\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:\n",
    "    MODEL_SAVE_DIR=\"./hf_results/\"\n",
    "    MAX_LENGTH=512\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    TWEET_COL = \"processed_text\"\n",
    "    RANDOM_STATE = 42\n",
    "    BATCH_SIZE = 16\n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_EPOCHS = 3\n",
    "    NUM_WORKERS = 8\n",
    "    TRANSFORMER_CHECKPOINT = \"bert-base-uncased\"\n",
    "    # The hidden_size of the output of the last layer of the transformer model used\n",
    "    TRANSFORMER_OUT_SIZE = 768\n",
    "    PAD_TOKEN_ID = 0\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False    \n",
    "    PATIENCE = 5        \n",
    "    # model hyperparameters\n",
    "    MODEL_HPARAMS = {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"warmup_steps\": 0\n",
    "    }\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "set_seed(Config.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 7613\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold CV\n",
    "Split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "on the data. We use stratified kfold if the target distribution is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold_dataframe(df, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df.sample(frac=1, random_state=Config.RANDOM_STATE).reset_index(drop=True)\n",
    "    y = df[\"target\"].values\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "    # stratification is done on the basis of y labels, a placeholder for X is sufficient\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_idx, \"kfold\"] = fold\n",
    "    return df\n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, num_folds=Config.NUM_FOLDS)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bk_anupam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct):\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "    return text\n",
    "\n",
    "def process_tweet(df, text, keyword):\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n",
    "    processed_text = []\n",
    "    stop = stopwords.words(\"english\")\n",
    "    for tweet, keyword in zip(df[text], df[keyword]):\n",
    "        tweets_clean = []        \n",
    "        # remove stock market tickers like $GE        \n",
    "        tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash #, @, ... sign from the word\n",
    "        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n",
    "        tweet = clean_special_chars(tweet, punct)\n",
    "        # remove junk characters which don't have an ascii code\n",
    "        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # tokenize tweets        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        for word in tweet_tokens:\n",
    "            # remove stopwords and punctuation\n",
    "            #if (word.isalpha() and len(word) > 2 and word not in stop and word not in string.punctuation):\n",
    "                #stem_word = stemmer.stem(word)  # stemming word            \n",
    "                #lem_word = lemmatizer.lemmatize(word)\n",
    "                #tweets_clean.append(lem_word) \n",
    "                tweets_clean.append(word)\n",
    "        processed_text.append(\" \".join(tweets_clean))        \n",
    "    df['processed_text'] = np.array(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prcsd_tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>73</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Sheffield Township, Ohio</td>\n",
       "      <td>Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>deputies man shot before brighton home set ablaze</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>74</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>India</td>\n",
       "      <td>Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>man wife get six years jail for setting ablaze niece</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                  location  \\\n",
       "50  73  ablaze  Sheffield Township, Ohio   \n",
       "51  74  ablaze                     India   \n",
       "\n",
       "                                                                            text  \\\n",
       "50     Deputies: Man shot before Brighton home set ablaze http://t.co/gWNRhMSO8k   \n",
       "51  Man wife get six years jail for setting ablaze niece\\nhttp://t.co/eV1ahOUCZA   \n",
       "\n",
       "    target  kfold                                        processed_text  \\\n",
       "50       1      2     deputies man shot before brighton home set ablaze   \n",
       "51       1      0  man wife get six years jail for setting ablaze niece   \n",
       "\n",
       "    prcsd_tweet_len  \n",
       "50                8  \n",
       "51               10  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values\n",
    "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\n",
    "df_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\n",
    "process_tweet(df_train, 'text', \"keyword\")\n",
    "process_tweet(df_test, 'text', \"keyword\")\n",
    "# length of the processed tweet\n",
    "df_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\n",
    "df_train.iloc[50:52, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for transformer model\n",
    "We use hugging face Dataset library to create a custom dataset from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(Config.TRANSFORMER_CHECKPOINT)\n",
    "# DataCollatorWithPadding pads each batch to the longest sequence length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row of the huggingface Dataset \n",
    "def tokenize_tweets(tokenizer, with_labels, row):\n",
    "    result = tokenizer(row[Config.TWEET_COL], padding=False, truncation=True)\n",
    "    if with_labels:\n",
    "        result[\"labels\"] = row[\"target\"]\n",
    "    return result\n",
    "\n",
    "preprocess_train_data = partial(tokenize_tweets, tokenizer, True)  \n",
    "preprocess_test_data = partial(tokenize_tweets, tokenizer, False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train and validation dataset and dataloaders for a fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    ds_train_raw = Dataset.from_pandas(train_df)\n",
    "    ds_valid_raw = Dataset.from_pandas(valid_df)\n",
    "    raw_ds_col_names = ds_train_raw.column_names    \n",
    "    ds_train = ds_train_raw.map(preprocess_train_data, batched=True, remove_columns=raw_ds_col_names)\n",
    "    ds_valid = ds_valid_raw.map(preprocess_train_data, batched=True, remove_columns=raw_ds_col_names)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_valid, ds_train, ds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "def get_optimizer(model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": Config.MODEL_HPARAMS[\"weight_decay\"],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    return AdamW(optimizer_grouped_parameters, lr=Config.MODEL_HPARAMS[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler = get_scheduler(\n",
    "#         name=\"linear\",\n",
    "#         optimizer=optimizer,\n",
    "#         num_warmup_steps=Config.MODEL_HPARAMS[\"warmup_steps\"],\n",
    "#         num_training_steps=num_train_steps\n",
    "#     )\n",
    "\n",
    "def get_lr_scheduler(optimizer, dl_train):\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(len(dl_train) / Config.GRADIENT_ACCUMULATION_STEPS)\n",
    "    num_train_steps = Config.NUM_EPOCHS * num_update_steps_per_epoch\n",
    "    print(f\"num_update_steps_per_epoch = {num_update_steps_per_epoch}\")\n",
    "    print(f\"num_train_steps = {num_train_steps}\")\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=Config.MODEL_HPARAMS[\"warmup_steps\"],\n",
    "            num_training_steps=num_train_steps,\n",
    "        )\n",
    "    return lr_scheduler        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, model, optimizer, lr_scheduler, train_dataloader):\n",
    "    progress_bar = tqdm(range(len(train_dataloader)))\n",
    "    train_loss_epoch = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss      \n",
    "        train_loss_epoch.append(loss.item())  \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)        \n",
    "    train_loss_mean = np.mean(train_loss_epoch)\n",
    "    return train_loss_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(epoch, model, val_dataloader, val_metric):\n",
    "    progress_bar = tqdm(range(len(val_dataloader)))\n",
    "    val_loss_epoch = []\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):    \n",
    "            batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}           \n",
    "            outputs = model(**batch)\n",
    "            val_loss_epoch.append(outputs.loss.item())\n",
    "            predictions = outputs.logits.argmax(dim=-1) \n",
    "            val_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    val_metric_epoch = val_metric.compute()\n",
    "    val_metric_epoch = round(val_metric_epoch['accuracy'], 4)\n",
    "    val_loss_mean = np.mean(val_loss_epoch)    \n",
    "    return val_loss_mean, val_metric_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_train_evaluate(fold):    \n",
    "    fold_best_model_path = \"\"\n",
    "    fold_str = f\"fold_{fold}\"\n",
    "    print(f\"Running fold {fold}\")\n",
    "    # we use validation loss as the criteria to save best model for a CV fold\n",
    "    fold_val_loss_min = np.inf\n",
    "    dl_train, dl_valid, ds_train, ds_valid = get_fold_dls(fold, df_train)\n",
    "    print(f\"Created data loaders for {fold_str}\")\n",
    "    config = BertConfig.from_pretrained(Config.TRANSFORMER_CHECKPOINT, num_labels=Config.OUT_SIZE)\n",
    "    model = BertForSequenceClassification.from_pretrained(Config.TRANSFORMER_CHECKPOINT,config=config)\n",
    "    model.to(Config.DEVICE)  \n",
    "    optimizer = get_optimizer(model)\n",
    "    lr_scheduler = get_lr_scheduler(optimizer, dl_train)\n",
    "    val_metric = load_metric(Config.MODEL_EVAL_METRIC)    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        print(f\"Running training for epoch {epoch+1}\")\n",
    "        epoch_train_loss = train_fn(epoch+1, model, optimizer, lr_scheduler, dl_train)        \n",
    "        print(f\"Running validation for epoch {epoch+1}\")\n",
    "        epoch_val_loss, epoch_val_metric = eval_fn(epoch+1, model, dl_valid, val_metric)\n",
    "        print(f\"EPOCH {epoch+1}: \")\n",
    "        print(f\"train_loss: {round(epoch_train_loss, 4)}\")            \n",
    "        print(f\"val_loss: {round(epoch_val_loss, 4)}\")\n",
    "        print(f\"{Config.MODEL_EVAL_METRIC}: {epoch_val_metric}\")\n",
    "        if epoch_val_loss < fold_val_loss_min:\n",
    "            print(f\"Validation loss decreased from \" +\n",
    "                  f\"{round(fold_val_loss_min, 6)} --> {round(epoch_val_loss, 6)}. Saving model...\")\n",
    "            fold_best_model_path = Config.MODEL_SAVE_DIR + fold_str                  \n",
    "            model.save_pretrained(fold_best_model_path)\n",
    "            fold_val_loss_min = epoch_val_loss            \n",
    "\n",
    "    del optimizer, lr_scheduler, model   \n",
    "    return fold_val_loss_min, fold_best_model_path      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5475acde1b34f51934417616e9dec35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3221513db4e64f139c67bb5290aa464a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data loaders for fold_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_update_steps_per_epoch = 381\n",
      "num_train_steps = 1143\n",
      "Running training for epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688e3a3e43304b17b6edf20ccdb9dd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation for epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ff1206bdfd4a85bde42a0d937e25bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: \n",
      "train_loss: 0.4456\n",
      "val_loss: 0.3691\n",
      "accuracy: 0.8345\n",
      "Validation loss decreased from inf --> 0.369067. Saving model...\n",
      "Running training for epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11facbf099334dfdbf90f75e216f7b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation for epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6342f996a1746abb22fb58ae5d5e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: \n",
      "train_loss: 0.3114\n",
      "val_loss: 0.4295\n",
      "accuracy: 0.8188\n",
      "Running training for epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5234e98ff3254f0da21d3405f7f0d586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation for epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bda32cb1c5f447ab4610ee9cbd9837b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: \n",
      "train_loss: 0.2266\n",
      "val_loss: 0.4414\n",
      "accuracy: 0.828\n"
     ]
    }
   ],
   "source": [
    "fold_results = []\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    fold_val_loss_min, fold_best_model_path = fold_train_evaluate(fold)\n",
    "    fold_results.append((fold_val_loss_min, fold_best_model_path))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on the test set using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best performing model \n",
    "fold_results_sorted = sorted(fold_results, key=lambda x:x[0])\n",
    "best_model_across_folds = fold_results_sorted[0][1]\n",
    "best_model = BertForSequenceClassification.from_pretrained(best_model_across_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f9f06f90e24b099f74a9f7195a9665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data loader for test data\n",
    "ds_test_raw = Dataset.from_pandas(df_test)\n",
    "ds_test_raw_col_names = ds_test_raw.column_names\n",
    "ds_test = ds_test_raw.map(preprocess_test_data, batched=True, remove_columns=ds_test_raw_col_names)\n",
    "dl_test = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "# perform predictions on test data\n",
    "test_preds = []\n",
    "best_model.to(Config.DEVICE)\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(dl_test)):\n",
    "        batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}           \n",
    "        outputs = best_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)         \n",
    "        test_preds.extend(predictions.cpu().detach().numpy().tolist())\n",
    "\n",
    "print(f\"Completed prediction for {len(test_preds)} test rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the submission file\n",
    "df_submission = pd.read_csv(DATA_PATH + 'submission.csv')\n",
    "df_submission['target']= test_preds\n",
    "df_submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
